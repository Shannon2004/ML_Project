{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10165785,"sourceType":"datasetVersion","datasetId":6277707}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Neural Network","metadata":{}},{"cell_type":"markdown","source":"### Load the Processed Training Data","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n# torch is a powerful deep learning framework for building and training neural networks.\nimport torch\n# torch.nn provides modules and classes to help create neural network layers.\nimport torch.nn as nn\n# contains optimization algorithms like SGD, Adam, etc., for training models.\nimport torch.optim as optim\nimport random\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score","metadata":{"id":"_noeVsl6Wcpr","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T07:09:00.106537Z","iopub.execute_input":"2024-12-12T07:09:00.107315Z","iopub.status.idle":"2024-12-12T07:09:04.477036Z","shell.execute_reply.started":"2024-12-12T07:09:00.107266Z","shell.execute_reply":"2024-12-12T07:09:04.476253Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# CUDA (Compute Unified Device Architecture) is a parallel computing platform and application programming interface\n# (API) created by NVIDIA. It allows developers to use the GPU (Graphics Processing Unit) for general-purpose computing tasks,\n# accelerating computations significantly compared to a CPU.\n\n# A tensor is a multi-dimensional array, similar to numpy arrays, but with additional capabilities designed for deep learning and GPU acceleration.","metadata":{"id":"3Y6to4Y0Ymaf","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T07:16:50.070721Z","iopub.execute_input":"2024-12-12T07:16:50.071595Z","iopub.status.idle":"2024-12-12T07:16:50.075259Z","shell.execute_reply.started":"2024-12-12T07:16:50.071558Z","shell.execute_reply":"2024-12-12T07:16:50.074449Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Check if CUDA is available\nprint(\"CUDA available:\", torch.cuda.is_available())\n\n# If available, get the GPU name\nif torch.cuda.is_available():\n    print(\"GPU Name:\", torch.cuda.get_device_name(0))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BHkl7_92W8Pu","outputId":"9814b716-b68e-4bd2-b293-af8349cf807f","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T07:16:55.597973Z","iopub.execute_input":"2024-12-12T07:16:55.598551Z","iopub.status.idle":"2024-12-12T07:16:55.712798Z","shell.execute_reply.started":"2024-12-12T07:16:55.598523Z","shell.execute_reply":"2024-12-12T07:16:55.711983Z"}},"outputs":[{"name":"stdout","text":"CUDA available: True\nGPU Name: Tesla T4\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Load the processed data for training","metadata":{"id":"Rwz5bysIYL_t","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T07:17:00.316700Z","iopub.execute_input":"2024-12-12T07:17:00.317444Z","iopub.status.idle":"2024-12-12T07:17:00.321147Z","shell.execute_reply.started":"2024-12-12T07:17:00.317410Z","shell.execute_reply":"2024-12-12T07:17:00.320257Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"df=pd.read_csv('/kaggle/input/ml-assignment/Processed_train.csv')","metadata":{"id":"FK3VesQrXjdy","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T07:17:04.078898Z","iopub.execute_input":"2024-12-12T07:17:04.079510Z","iopub.status.idle":"2024-12-12T07:17:04.534636Z","shell.execute_reply.started":"2024-12-12T07:17:04.079479Z","shell.execute_reply":"2024-12-12T07:17:04.533926Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"df.head(5)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":226},"id":"VYJKZnu1X70p","outputId":"765f3f51-da25-4c0c-a5ef-94a908157f64","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T07:17:07.482509Z","iopub.execute_input":"2024-12-12T07:17:07.483329Z","iopub.status.idle":"2024-12-12T07:17:07.503621Z","shell.execute_reply.started":"2024-12-12T07:17:07.483297Z","shell.execute_reply":"2024-12-12T07:17:07.502861Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"   Age  Income  LoanAmount  CreditScore  MonthsEmployed  NumCreditLines  \\\n0   18  137576      209136          846              26               2   \n1   47   57194        5970          748              30               2   \n2   26   84328       95065          453               7               2   \n3   53   49795      229582          533             107               3   \n4   49  115450       22072          840               0               4   \n\n   InterestRate  LoanTerm  DTIRatio  Education  EmploymentType  MaritalStatus  \\\n0         10.47        60      0.81          1               3              2   \n1         19.72        36      0.73          1               1              1   \n2         24.25        12      0.45          3               3              3   \n3         14.44        60      0.17          2               3              2   \n4         24.48        12      0.11          2               2              2   \n\n   HasMortgage  HasDependents  LoanPurpose  HasCoSigner  Default  \n0            1              0            0            0        0  \n1            0              1            1            0        0  \n2            0              0            3            1        0  \n3            1              0            2            1        1  \n4            0              1            1            1        0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Age</th>\n      <th>Income</th>\n      <th>LoanAmount</th>\n      <th>CreditScore</th>\n      <th>MonthsEmployed</th>\n      <th>NumCreditLines</th>\n      <th>InterestRate</th>\n      <th>LoanTerm</th>\n      <th>DTIRatio</th>\n      <th>Education</th>\n      <th>EmploymentType</th>\n      <th>MaritalStatus</th>\n      <th>HasMortgage</th>\n      <th>HasDependents</th>\n      <th>LoanPurpose</th>\n      <th>HasCoSigner</th>\n      <th>Default</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>18</td>\n      <td>137576</td>\n      <td>209136</td>\n      <td>846</td>\n      <td>26</td>\n      <td>2</td>\n      <td>10.47</td>\n      <td>60</td>\n      <td>0.81</td>\n      <td>1</td>\n      <td>3</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>47</td>\n      <td>57194</td>\n      <td>5970</td>\n      <td>748</td>\n      <td>30</td>\n      <td>2</td>\n      <td>19.72</td>\n      <td>36</td>\n      <td>0.73</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>26</td>\n      <td>84328</td>\n      <td>95065</td>\n      <td>453</td>\n      <td>7</td>\n      <td>2</td>\n      <td>24.25</td>\n      <td>12</td>\n      <td>0.45</td>\n      <td>3</td>\n      <td>3</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>53</td>\n      <td>49795</td>\n      <td>229582</td>\n      <td>533</td>\n      <td>107</td>\n      <td>3</td>\n      <td>14.44</td>\n      <td>60</td>\n      <td>0.17</td>\n      <td>2</td>\n      <td>3</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>49</td>\n      <td>115450</td>\n      <td>22072</td>\n      <td>840</td>\n      <td>0</td>\n      <td>4</td>\n      <td>24.48</td>\n      <td>12</td>\n      <td>0.11</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"print(df.shape)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-cBEw-5afCIX","outputId":"b2dee227-1d7d-4cff-cd50-44c9dbf4f89f","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T07:17:10.823173Z","iopub.execute_input":"2024-12-12T07:17:10.823803Z","iopub.status.idle":"2024-12-12T07:17:10.828083Z","shell.execute_reply.started":"2024-12-12T07:17:10.823769Z","shell.execute_reply":"2024-12-12T07:17:10.827178Z"}},"outputs":[{"name":"stdout","text":"(204277, 17)\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"### Need to train test split","metadata":{}},{"cell_type":"code","source":"# Set random seed for reproducibility\nnp.random.seed(42)\ntorch.manual_seed(42)  # For CPU\ntorch.cuda.manual_seed_all(42)  # For GPU\nrandom.seed(42)\n\n# Split features (X) and target (Y)\nX = df.drop(columns=['Default'])  # Drop the target column from features\nY = df['Default']                 # Target column\n\n# Split the dataset into training and test sets\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)","metadata":{"id":"7LlPvA7TcAWU","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T07:17:41.756531Z","iopub.execute_input":"2024-12-12T07:17:41.757157Z","iopub.status.idle":"2024-12-12T07:17:41.808673Z","shell.execute_reply.started":"2024-12-12T07:17:41.757128Z","shell.execute_reply":"2024-12-12T07:17:41.807898Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"### Scale the features","metadata":{"id":"_G-WRQaUdL0d"}},{"cell_type":"code","source":"# Scaling the input features\nscaler= StandardScaler()\nscaled_X_train=scaler.fit_transform(X_train)\nscaled_X_test= scaler.transform(X_test)","metadata":{"id":"dngTAg5QeBD2","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T07:17:45.547813Z","iopub.execute_input":"2024-12-12T07:17:45.548586Z","iopub.status.idle":"2024-12-12T07:17:45.596338Z","shell.execute_reply.started":"2024-12-12T07:17:45.548551Z","shell.execute_reply":"2024-12-12T07:17:45.595642Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"print(type(Y_train))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WgXjq958eHv8","outputId":"dd943bd7-ebb0-4967-cf58-b60e94f75781","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T07:17:48.235269Z","iopub.execute_input":"2024-12-12T07:17:48.235642Z","iopub.status.idle":"2024-12-12T07:17:48.240744Z","shell.execute_reply.started":"2024-12-12T07:17:48.235611Z","shell.execute_reply":"2024-12-12T07:17:48.239728Z"}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.series.Series'>\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"print(type(scaled_X_train))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3cIT98XEgCf3","outputId":"f46c67b1-ba3c-4e1a-8fe2-5386e9b2ef7a","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T07:17:50.263919Z","iopub.execute_input":"2024-12-12T07:17:50.264232Z","iopub.status.idle":"2024-12-12T07:17:50.268821Z","shell.execute_reply.started":"2024-12-12T07:17:50.264205Z","shell.execute_reply":"2024-12-12T07:17:50.267884Z"}},"outputs":[{"name":"stdout","text":"<class 'numpy.ndarray'>\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"### Convert to tensors","metadata":{}},{"cell_type":"code","source":"# Convert numpy arrays to pytorch tensors\nX_train_tensor=torch.tensor(scaled_X_train,dtype=torch.float32)\nX_test_tensor=torch.tensor(scaled_X_test,dtype=torch.float32)\nY_train_tensor=torch.tensor(Y_train.values,dtype=torch.float32)\nY_test_tensor=torch.tensor(Y_test.values,dtype=torch.float32)","metadata":{"id":"cWPQeHWQeLEF","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T07:17:54.818535Z","iopub.execute_input":"2024-12-12T07:17:54.818888Z","iopub.status.idle":"2024-12-12T07:17:54.856161Z","shell.execute_reply.started":"2024-12-12T07:17:54.818855Z","shell.execute_reply":"2024-12-12T07:17:54.855235Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"print(X_train_tensor.shape)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KNXBBDl-ge29","outputId":"cc0919c3-e943-4b72-8ff6-e92782e50f23","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T07:17:57.448654Z","iopub.execute_input":"2024-12-12T07:17:57.449024Z","iopub.status.idle":"2024-12-12T07:17:57.454107Z","shell.execute_reply.started":"2024-12-12T07:17:57.448974Z","shell.execute_reply":"2024-12-12T07:17:57.453091Z"}},"outputs":[{"name":"stdout","text":"torch.Size([163421, 16])\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"Y_train_tensor=Y_train_tensor.unsqueeze(1)\nY_test_tensor=Y_test_tensor.unsqueeze(1)","metadata":{"id":"cBMlAWFLhSJP","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T07:17:59.655653Z","iopub.execute_input":"2024-12-12T07:17:59.655997Z","iopub.status.idle":"2024-12-12T07:17:59.668497Z","shell.execute_reply.started":"2024-12-12T07:17:59.655966Z","shell.execute_reply":"2024-12-12T07:17:59.667577Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"print(Y_train_tensor.shape)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HZXQ8jIlhaLX","outputId":"9a3e4fac-78c7-4197-946c-2481f43bf298","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T07:18:02.376820Z","iopub.execute_input":"2024-12-12T07:18:02.377706Z","iopub.status.idle":"2024-12-12T07:18:02.382305Z","shell.execute_reply.started":"2024-12-12T07:18:02.377667Z","shell.execute_reply":"2024-12-12T07:18:02.381365Z"}},"outputs":[{"name":"stdout","text":"torch.Size([163421, 1])\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# define the model\n# Note, Dropout is a regularization technique that is used to prevent overfitting. During training, Dropout randomly sets a fraction of input units to 0.","metadata":{"id":"BIJ8YHI6hdPW","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T07:18:04.917040Z","iopub.execute_input":"2024-12-12T07:18:04.917391Z","iopub.status.idle":"2024-12-12T07:18:04.921276Z","shell.execute_reply.started":"2024-12-12T07:18:04.917346Z","shell.execute_reply":"2024-12-12T07:18:04.920453Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"### Defining the first model","metadata":{}},{"cell_type":"code","source":"model=nn.Sequential(\n    # Defining a 3 layer neural network - 2 hidden + 1 output layer. We use ReLU activation function for the hidden layers and Sigmoid activation function for the\n    # output layer since it is a binary classification problem\n    nn.Linear(16,128), # input layer to first hidden layer (16 -> 128 neurons)\n    nn.ReLU(), # Rectified Linear Unit Activation Function\n    nn.Linear(128,64), # first hidden layer to second hidden layer (128 -> 64 neurons)\n    nn.ReLU(), # Rectified Linear Unit Activation Function again\n    nn.Linear(64,1), # second hidden layer to output layer (64 -> 1 neuron)\n    nn.Sigmoid() # Sigmoid activation function since it is binary classification\n)","metadata":{"id":"9hyK5bLkiB5X","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T08:17:10.531700Z","iopub.execute_input":"2024-12-12T08:17:10.532415Z","iopub.status.idle":"2024-12-12T08:17:10.538633Z","shell.execute_reply.started":"2024-12-12T08:17:10.532352Z","shell.execute_reply":"2024-12-12T08:17:10.537771Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"print('Model:',model)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YBQ9AO_8ij3t","outputId":"c3b1bd33-4f6a-445d-cb32-be8f4f9a004c","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T08:17:19.214550Z","iopub.execute_input":"2024-12-12T08:17:19.215165Z","iopub.status.idle":"2024-12-12T08:17:19.219604Z","shell.execute_reply.started":"2024-12-12T08:17:19.215132Z","shell.execute_reply":"2024-12-12T08:17:19.218687Z"}},"outputs":[{"name":"stdout","text":"Model: Sequential(\n  (0): Linear(in_features=16, out_features=128, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=128, out_features=64, bias=True)\n  (3): ReLU()\n  (4): Linear(in_features=64, out_features=1, bias=True)\n  (5): Sigmoid()\n)\n","output_type":"stream"}],"execution_count":46},{"cell_type":"markdown","source":"### Define the loss function and optimizer","metadata":{"id":"K3RDImi1imI7"}},{"cell_type":"code","source":"# Since it is a binary classification problem, we will use the binary cross entropy loss function","metadata":{"id":"G67df0CIjSa0","trusted":true,"execution":{"iopub.status.busy":"2024-12-11T05:48:13.487648Z","iopub.execute_input":"2024-12-11T05:48:13.487991Z","iopub.status.idle":"2024-12-11T05:48:13.491933Z","shell.execute_reply.started":"2024-12-11T05:48:13.487959Z","shell.execute_reply":"2024-12-11T05:48:13.490958Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"loss_function = nn.BCELoss()","metadata":{"id":"fBmFbnXojX17","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T08:17:21.976867Z","iopub.execute_input":"2024-12-12T08:17:21.977236Z","iopub.status.idle":"2024-12-12T08:17:21.981548Z","shell.execute_reply.started":"2024-12-12T08:17:21.977203Z","shell.execute_reply":"2024-12-12T08:17:21.980607Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"# Need to define optimizer as well. An optimizer is an algorithm you use to adjust model weights progressively.\noptimizer = optim.Adam(model.parameters(), lr=0.01)","metadata":{"id":"xEl8bsrIjejV","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T08:17:23.798561Z","iopub.execute_input":"2024-12-12T08:17:23.799483Z","iopub.status.idle":"2024-12-12T08:17:24.516843Z","shell.execute_reply.started":"2024-12-12T08:17:23.799447Z","shell.execute_reply":"2024-12-12T08:17:24.516053Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Move the model to GPU\nmodel = model.to(device)\n\n# Move the data to GPU\nX_train_tensor = X_train_tensor.to(device)\nX_test_tensor = X_test_tensor.to(device)\nY_train_tensor = Y_train_tensor.to(device)\nY_test_tensor = Y_test_tensor.to(device)\n","metadata":{"id":"TofTY5a1j6Vn","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T08:17:26.018753Z","iopub.execute_input":"2024-12-12T08:17:26.019396Z","iopub.status.idle":"2024-12-12T08:17:26.025559Z","shell.execute_reply.started":"2024-12-12T08:17:26.019332Z","shell.execute_reply":"2024-12-12T08:17:26.024660Z"}},"outputs":[],"execution_count":49},{"cell_type":"markdown","source":"### Training the Model","metadata":{}},{"cell_type":"code","source":"# training the model\nnum_epochs = 5000  # Number of epochs for training\n\nfor epoch in range(num_epochs):\n    model.train()  # Set model to training mode\n\n    # Forward pass\n    outputs = model(X_train_tensor)  # Get predictions from model\n    loss = loss_function(outputs, Y_train_tensor)  # Calculate loss\n\n    # Backward pass\n    optimizer.zero_grad()  # Clear previous gradients\n    loss.backward()  # Backpropagate gradients\n    optimizer.step()  # Update weights\n\n    # Print loss every 10 epochs\n    if (epoch + 1) % 10 == 0:\n        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qdlRJozIkaCo","outputId":"3ffb871c-4757-4728-c993-2bee132786f2","trusted":true,"execution":{"iopub.status.busy":"2024-12-11T09:57:31.945867Z","iopub.execute_input":"2024-12-11T09:57:31.946719Z","iopub.status.idle":"2024-12-11T09:58:14.154977Z","shell.execute_reply.started":"2024-12-11T09:57:31.946683Z","shell.execute_reply":"2024-12-11T09:58:14.154097Z"}},"outputs":[{"name":"stdout","text":"Epoch [10/5000], Loss: 0.3305\nEpoch [20/5000], Loss: 0.3289\nEpoch [30/5000], Loss: 0.3232\nEpoch [40/5000], Loss: 0.3195\nEpoch [50/5000], Loss: 0.3181\nEpoch [60/5000], Loss: 0.3174\nEpoch [70/5000], Loss: 0.3167\nEpoch [80/5000], Loss: 0.3159\nEpoch [90/5000], Loss: 0.3152\nEpoch [100/5000], Loss: 0.3145\nEpoch [110/5000], Loss: 0.3139\nEpoch [120/5000], Loss: 0.3133\nEpoch [130/5000], Loss: 0.3128\nEpoch [140/5000], Loss: 0.3124\nEpoch [150/5000], Loss: 0.3121\nEpoch [160/5000], Loss: 0.3118\nEpoch [170/5000], Loss: 0.3115\nEpoch [180/5000], Loss: 0.3112\nEpoch [190/5000], Loss: 0.3110\nEpoch [200/5000], Loss: 0.3107\nEpoch [210/5000], Loss: 0.3104\nEpoch [220/5000], Loss: 0.3101\nEpoch [230/5000], Loss: 0.3098\nEpoch [240/5000], Loss: 0.3095\nEpoch [250/5000], Loss: 0.3092\nEpoch [260/5000], Loss: 0.3089\nEpoch [270/5000], Loss: 0.3086\nEpoch [280/5000], Loss: 0.3083\nEpoch [290/5000], Loss: 0.3080\nEpoch [300/5000], Loss: 0.3077\nEpoch [310/5000], Loss: 0.3073\nEpoch [320/5000], Loss: 0.3070\nEpoch [330/5000], Loss: 0.3066\nEpoch [340/5000], Loss: 0.3062\nEpoch [350/5000], Loss: 0.3058\nEpoch [360/5000], Loss: 0.3054\nEpoch [370/5000], Loss: 0.3053\nEpoch [380/5000], Loss: 0.3049\nEpoch [390/5000], Loss: 0.3044\nEpoch [400/5000], Loss: 0.3041\nEpoch [410/5000], Loss: 0.3041\nEpoch [420/5000], Loss: 0.3035\nEpoch [430/5000], Loss: 0.3036\nEpoch [440/5000], Loss: 0.3029\nEpoch [450/5000], Loss: 0.3030\nEpoch [460/5000], Loss: 0.3028\nEpoch [470/5000], Loss: 0.3025\nEpoch [480/5000], Loss: 0.3023\nEpoch [490/5000], Loss: 0.3020\nEpoch [500/5000], Loss: 0.3016\nEpoch [510/5000], Loss: 0.3014\nEpoch [520/5000], Loss: 0.3016\nEpoch [530/5000], Loss: 0.3020\nEpoch [540/5000], Loss: 0.3020\nEpoch [550/5000], Loss: 0.3015\nEpoch [560/5000], Loss: 0.3011\nEpoch [570/5000], Loss: 0.3006\nEpoch [580/5000], Loss: 0.3003\nEpoch [590/5000], Loss: 0.3002\nEpoch [600/5000], Loss: 0.3001\nEpoch [610/5000], Loss: 0.3005\nEpoch [620/5000], Loss: 0.3008\nEpoch [630/5000], Loss: 0.3004\nEpoch [640/5000], Loss: 0.2997\nEpoch [650/5000], Loss: 0.2996\nEpoch [660/5000], Loss: 0.3002\nEpoch [670/5000], Loss: 0.3003\nEpoch [680/5000], Loss: 0.2999\nEpoch [690/5000], Loss: 0.2992\nEpoch [700/5000], Loss: 0.2993\nEpoch [710/5000], Loss: 0.3000\nEpoch [720/5000], Loss: 0.2995\nEpoch [730/5000], Loss: 0.2989\nEpoch [740/5000], Loss: 0.2991\nEpoch [750/5000], Loss: 0.2996\nEpoch [760/5000], Loss: 0.2997\nEpoch [770/5000], Loss: 0.2989\nEpoch [780/5000], Loss: 0.2985\nEpoch [790/5000], Loss: 0.2991\nEpoch [800/5000], Loss: 0.2993\nEpoch [810/5000], Loss: 0.2989\nEpoch [820/5000], Loss: 0.2982\nEpoch [830/5000], Loss: 0.2985\nEpoch [840/5000], Loss: 0.2993\nEpoch [850/5000], Loss: 0.2984\nEpoch [860/5000], Loss: 0.2979\nEpoch [870/5000], Loss: 0.2990\nEpoch [880/5000], Loss: 0.2987\nEpoch [890/5000], Loss: 0.2979\nEpoch [900/5000], Loss: 0.2979\nEpoch [910/5000], Loss: 0.2987\nEpoch [920/5000], Loss: 0.2979\nEpoch [930/5000], Loss: 0.2976\nEpoch [940/5000], Loss: 0.2985\nEpoch [950/5000], Loss: 0.2975\nEpoch [960/5000], Loss: 0.2977\nEpoch [970/5000], Loss: 0.2983\nEpoch [980/5000], Loss: 0.2974\nEpoch [990/5000], Loss: 0.2976\nEpoch [1000/5000], Loss: 0.2983\nEpoch [1010/5000], Loss: 0.2975\nEpoch [1020/5000], Loss: 0.2969\nEpoch [1030/5000], Loss: 0.2981\nEpoch [1040/5000], Loss: 0.2973\nEpoch [1050/5000], Loss: 0.2968\nEpoch [1060/5000], Loss: 0.2978\nEpoch [1070/5000], Loss: 0.2976\nEpoch [1080/5000], Loss: 0.2966\nEpoch [1090/5000], Loss: 0.2973\nEpoch [1100/5000], Loss: 0.2976\nEpoch [1110/5000], Loss: 0.2964\nEpoch [1120/5000], Loss: 0.2969\nEpoch [1130/5000], Loss: 0.2978\nEpoch [1140/5000], Loss: 0.2967\nEpoch [1150/5000], Loss: 0.2962\nEpoch [1160/5000], Loss: 0.2977\nEpoch [1170/5000], Loss: 0.2964\nEpoch [1180/5000], Loss: 0.2960\nEpoch [1190/5000], Loss: 0.2965\nEpoch [1200/5000], Loss: 0.2970\nEpoch [1210/5000], Loss: 0.2964\nEpoch [1220/5000], Loss: 0.2957\nEpoch [1230/5000], Loss: 0.2968\nEpoch [1240/5000], Loss: 0.2969\nEpoch [1250/5000], Loss: 0.2958\nEpoch [1260/5000], Loss: 0.2955\nEpoch [1270/5000], Loss: 0.2970\nEpoch [1280/5000], Loss: 0.2958\nEpoch [1290/5000], Loss: 0.2953\nEpoch [1300/5000], Loss: 0.2965\nEpoch [1310/5000], Loss: 0.2959\nEpoch [1320/5000], Loss: 0.2950\nEpoch [1330/5000], Loss: 0.2962\nEpoch [1340/5000], Loss: 0.2958\nEpoch [1350/5000], Loss: 0.2949\nEpoch [1360/5000], Loss: 0.2955\nEpoch [1370/5000], Loss: 0.2960\nEpoch [1380/5000], Loss: 0.2948\nEpoch [1390/5000], Loss: 0.2952\nEpoch [1400/5000], Loss: 0.2960\nEpoch [1410/5000], Loss: 0.2948\nEpoch [1420/5000], Loss: 0.2945\nEpoch [1430/5000], Loss: 0.2956\nEpoch [1440/5000], Loss: 0.2954\nEpoch [1450/5000], Loss: 0.2943\nEpoch [1460/5000], Loss: 0.2949\nEpoch [1470/5000], Loss: 0.2954\nEpoch [1480/5000], Loss: 0.2940\nEpoch [1490/5000], Loss: 0.2951\nEpoch [1500/5000], Loss: 0.2957\nEpoch [1510/5000], Loss: 0.2943\nEpoch [1520/5000], Loss: 0.2938\nEpoch [1530/5000], Loss: 0.2955\nEpoch [1540/5000], Loss: 0.2945\nEpoch [1550/5000], Loss: 0.2936\nEpoch [1560/5000], Loss: 0.2944\nEpoch [1570/5000], Loss: 0.2950\nEpoch [1580/5000], Loss: 0.2941\nEpoch [1590/5000], Loss: 0.2934\nEpoch [1600/5000], Loss: 0.2948\nEpoch [1610/5000], Loss: 0.2946\nEpoch [1620/5000], Loss: 0.2933\nEpoch [1630/5000], Loss: 0.2936\nEpoch [1640/5000], Loss: 0.2950\nEpoch [1650/5000], Loss: 0.2936\nEpoch [1660/5000], Loss: 0.2931\nEpoch [1670/5000], Loss: 0.2948\nEpoch [1680/5000], Loss: 0.2941\nEpoch [1690/5000], Loss: 0.2931\nEpoch [1700/5000], Loss: 0.2930\nEpoch [1710/5000], Loss: 0.2942\nEpoch [1720/5000], Loss: 0.2941\nEpoch [1730/5000], Loss: 0.2928\nEpoch [1740/5000], Loss: 0.2931\nEpoch [1750/5000], Loss: 0.2946\nEpoch [1760/5000], Loss: 0.2928\nEpoch [1770/5000], Loss: 0.2927\nEpoch [1780/5000], Loss: 0.2945\nEpoch [1790/5000], Loss: 0.2934\nEpoch [1800/5000], Loss: 0.2924\nEpoch [1810/5000], Loss: 0.2927\nEpoch [1820/5000], Loss: 0.2939\nEpoch [1830/5000], Loss: 0.2932\nEpoch [1840/5000], Loss: 0.2921\nEpoch [1850/5000], Loss: 0.2933\nEpoch [1860/5000], Loss: 0.2938\nEpoch [1870/5000], Loss: 0.2922\nEpoch [1880/5000], Loss: 0.2921\nEpoch [1890/5000], Loss: 0.2939\nEpoch [1900/5000], Loss: 0.2931\nEpoch [1910/5000], Loss: 0.2920\nEpoch [1920/5000], Loss: 0.2920\nEpoch [1930/5000], Loss: 0.2934\nEpoch [1940/5000], Loss: 0.2924\nEpoch [1950/5000], Loss: 0.2916\nEpoch [1960/5000], Loss: 0.2930\nEpoch [1970/5000], Loss: 0.2930\nEpoch [1980/5000], Loss: 0.2915\nEpoch [1990/5000], Loss: 0.2926\nEpoch [2000/5000], Loss: 0.2932\nEpoch [2010/5000], Loss: 0.2915\nEpoch [2020/5000], Loss: 0.2915\nEpoch [2030/5000], Loss: 0.2934\nEpoch [2040/5000], Loss: 0.2917\nEpoch [2050/5000], Loss: 0.2911\nEpoch [2060/5000], Loss: 0.2929\nEpoch [2070/5000], Loss: 0.2921\nEpoch [2080/5000], Loss: 0.2910\nEpoch [2090/5000], Loss: 0.2915\nEpoch [2100/5000], Loss: 0.2926\nEpoch [2110/5000], Loss: 0.2910\nEpoch [2120/5000], Loss: 0.2917\nEpoch [2130/5000], Loss: 0.2925\nEpoch [2140/5000], Loss: 0.2911\nEpoch [2150/5000], Loss: 0.2906\nEpoch [2160/5000], Loss: 0.2914\nEpoch [2170/5000], Loss: 0.2925\nEpoch [2180/5000], Loss: 0.2908\nEpoch [2190/5000], Loss: 0.2905\nEpoch [2200/5000], Loss: 0.2922\nEpoch [2210/5000], Loss: 0.2920\nEpoch [2220/5000], Loss: 0.2909\nEpoch [2230/5000], Loss: 0.2903\nEpoch [2240/5000], Loss: 0.2907\nEpoch [2250/5000], Loss: 0.2921\nEpoch [2260/5000], Loss: 0.2912\nEpoch [2270/5000], Loss: 0.2901\nEpoch [2280/5000], Loss: 0.2902\nEpoch [2290/5000], Loss: 0.2907\nEpoch [2300/5000], Loss: 0.2915\nEpoch [2310/5000], Loss: 0.2899\nEpoch [2320/5000], Loss: 0.2922\nEpoch [2330/5000], Loss: 0.2913\nEpoch [2340/5000], Loss: 0.2899\nEpoch [2350/5000], Loss: 0.2900\nEpoch [2360/5000], Loss: 0.2918\nEpoch [2370/5000], Loss: 0.2906\nEpoch [2380/5000], Loss: 0.2896\nEpoch [2390/5000], Loss: 0.2906\nEpoch [2400/5000], Loss: 0.2914\nEpoch [2410/5000], Loss: 0.2898\nEpoch [2420/5000], Loss: 0.2900\nEpoch [2430/5000], Loss: 0.2906\nEpoch [2440/5000], Loss: 0.2893\nEpoch [2450/5000], Loss: 0.2915\nEpoch [2460/5000], Loss: 0.2893\nEpoch [2470/5000], Loss: 0.2913\nEpoch [2480/5000], Loss: 0.2916\nEpoch [2490/5000], Loss: 0.2897\nEpoch [2500/5000], Loss: 0.2890\nEpoch [2510/5000], Loss: 0.2911\nEpoch [2520/5000], Loss: 0.2902\nEpoch [2530/5000], Loss: 0.2891\nEpoch [2540/5000], Loss: 0.2893\nEpoch [2550/5000], Loss: 0.2891\nEpoch [2560/5000], Loss: 0.2891\nEpoch [2570/5000], Loss: 0.2886\nEpoch [2580/5000], Loss: 0.2918\nEpoch [2590/5000], Loss: 0.2924\nEpoch [2600/5000], Loss: 0.2905\nEpoch [2610/5000], Loss: 0.2886\nEpoch [2620/5000], Loss: 0.2885\nEpoch [2630/5000], Loss: 0.2892\nEpoch [2640/5000], Loss: 0.2890\nEpoch [2650/5000], Loss: 0.2896\nEpoch [2660/5000], Loss: 0.2882\nEpoch [2670/5000], Loss: 0.2893\nEpoch [2680/5000], Loss: 0.2899\nEpoch [2690/5000], Loss: 0.2887\nEpoch [2700/5000], Loss: 0.2880\nEpoch [2710/5000], Loss: 0.2898\nEpoch [2720/5000], Loss: 0.2902\nEpoch [2730/5000], Loss: 0.2881\nEpoch [2740/5000], Loss: 0.2884\nEpoch [2750/5000], Loss: 0.2904\nEpoch [2760/5000], Loss: 0.2882\nEpoch [2770/5000], Loss: 0.2880\nEpoch [2780/5000], Loss: 0.2907\nEpoch [2790/5000], Loss: 0.2888\nEpoch [2800/5000], Loss: 0.2876\nEpoch [2810/5000], Loss: 0.2901\nEpoch [2820/5000], Loss: 0.2891\nEpoch [2830/5000], Loss: 0.2876\nEpoch [2840/5000], Loss: 0.2893\nEpoch [2850/5000], Loss: 0.2881\nEpoch [2860/5000], Loss: 0.2874\nEpoch [2870/5000], Loss: 0.2890\nEpoch [2880/5000], Loss: 0.2893\nEpoch [2890/5000], Loss: 0.2879\nEpoch [2900/5000], Loss: 0.2875\nEpoch [2910/5000], Loss: 0.2891\nEpoch [2920/5000], Loss: 0.2873\nEpoch [2930/5000], Loss: 0.2875\nEpoch [2940/5000], Loss: 0.2891\nEpoch [2950/5000], Loss: 0.2872\nEpoch [2960/5000], Loss: 0.2882\nEpoch [2970/5000], Loss: 0.2886\nEpoch [2980/5000], Loss: 0.2870\nEpoch [2990/5000], Loss: 0.2875\nEpoch [3000/5000], Loss: 0.2892\nEpoch [3010/5000], Loss: 0.2876\nEpoch [3020/5000], Loss: 0.2867\nEpoch [3030/5000], Loss: 0.2891\nEpoch [3040/5000], Loss: 0.2870\nEpoch [3050/5000], Loss: 0.2869\nEpoch [3060/5000], Loss: 0.2886\nEpoch [3070/5000], Loss: 0.2871\nEpoch [3080/5000], Loss: 0.2867\nEpoch [3090/5000], Loss: 0.2891\nEpoch [3100/5000], Loss: 0.2875\nEpoch [3110/5000], Loss: 0.2862\nEpoch [3120/5000], Loss: 0.2884\nEpoch [3130/5000], Loss: 0.2877\nEpoch [3140/5000], Loss: 0.2862\nEpoch [3150/5000], Loss: 0.2885\nEpoch [3160/5000], Loss: 0.2867\nEpoch [3170/5000], Loss: 0.2863\nEpoch [3180/5000], Loss: 0.2885\nEpoch [3190/5000], Loss: 0.2877\nEpoch [3200/5000], Loss: 0.2859\nEpoch [3210/5000], Loss: 0.2880\nEpoch [3220/5000], Loss: 0.2870\nEpoch [3230/5000], Loss: 0.2862\nEpoch [3240/5000], Loss: 0.2857\nEpoch [3250/5000], Loss: 0.2871\nEpoch [3260/5000], Loss: 0.2892\nEpoch [3270/5000], Loss: 0.2887\nEpoch [3280/5000], Loss: 0.2885\nEpoch [3290/5000], Loss: 0.2860\nEpoch [3300/5000], Loss: 0.2856\nEpoch [3310/5000], Loss: 0.2856\nEpoch [3320/5000], Loss: 0.2861\nEpoch [3330/5000], Loss: 0.2861\nEpoch [3340/5000], Loss: 0.2868\nEpoch [3350/5000], Loss: 0.2854\nEpoch [3360/5000], Loss: 0.2871\nEpoch [3370/5000], Loss: 0.2875\nEpoch [3380/5000], Loss: 0.2858\nEpoch [3390/5000], Loss: 0.2856\nEpoch [3400/5000], Loss: 0.2878\nEpoch [3410/5000], Loss: 0.2864\nEpoch [3420/5000], Loss: 0.2852\nEpoch [3430/5000], Loss: 0.2877\nEpoch [3440/5000], Loss: 0.2870\nEpoch [3450/5000], Loss: 0.2852\nEpoch [3460/5000], Loss: 0.2862\nEpoch [3470/5000], Loss: 0.2869\nEpoch [3480/5000], Loss: 0.2850\nEpoch [3490/5000], Loss: 0.2871\nEpoch [3500/5000], Loss: 0.2872\nEpoch [3510/5000], Loss: 0.2854\nEpoch [3520/5000], Loss: 0.2851\nEpoch [3530/5000], Loss: 0.2878\nEpoch [3540/5000], Loss: 0.2852\nEpoch [3550/5000], Loss: 0.2850\nEpoch [3560/5000], Loss: 0.2872\nEpoch [3570/5000], Loss: 0.2862\nEpoch [3580/5000], Loss: 0.2847\nEpoch [3590/5000], Loss: 0.2887\nEpoch [3600/5000], Loss: 0.2856\nEpoch [3610/5000], Loss: 0.2846\nEpoch [3620/5000], Loss: 0.2872\nEpoch [3630/5000], Loss: 0.2846\nEpoch [3640/5000], Loss: 0.2846\nEpoch [3650/5000], Loss: 0.2877\nEpoch [3660/5000], Loss: 0.2850\nEpoch [3670/5000], Loss: 0.2853\nEpoch [3680/5000], Loss: 0.2860\nEpoch [3690/5000], Loss: 0.2853\nEpoch [3700/5000], Loss: 0.2844\nEpoch [3710/5000], Loss: 0.2843\nEpoch [3720/5000], Loss: 0.2879\nEpoch [3730/5000], Loss: 0.2882\nEpoch [3740/5000], Loss: 0.2850\nEpoch [3750/5000], Loss: 0.2845\nEpoch [3760/5000], Loss: 0.2853\nEpoch [3770/5000], Loss: 0.2841\nEpoch [3780/5000], Loss: 0.2871\nEpoch [3790/5000], Loss: 0.2856\nEpoch [3800/5000], Loss: 0.2840\nEpoch [3810/5000], Loss: 0.2851\nEpoch [3820/5000], Loss: 0.2847\nEpoch [3830/5000], Loss: 0.2841\nEpoch [3840/5000], Loss: 0.2864\nEpoch [3850/5000], Loss: 0.2839\nEpoch [3860/5000], Loss: 0.2878\nEpoch [3870/5000], Loss: 0.2844\nEpoch [3880/5000], Loss: 0.2844\nEpoch [3890/5000], Loss: 0.2856\nEpoch [3900/5000], Loss: 0.2859\nEpoch [3910/5000], Loss: 0.2839\nEpoch [3920/5000], Loss: 0.2844\nEpoch [3930/5000], Loss: 0.2860\nEpoch [3940/5000], Loss: 0.2839\nEpoch [3950/5000], Loss: 0.2845\nEpoch [3960/5000], Loss: 0.2836\nEpoch [3970/5000], Loss: 0.2838\nEpoch [3980/5000], Loss: 0.2851\nEpoch [3990/5000], Loss: 0.2847\nEpoch [4000/5000], Loss: 0.2834\nEpoch [4010/5000], Loss: 0.2859\nEpoch [4020/5000], Loss: 0.2834\nEpoch [4030/5000], Loss: 0.2867\nEpoch [4040/5000], Loss: 0.2852\nEpoch [4050/5000], Loss: 0.2838\nEpoch [4060/5000], Loss: 0.2834\nEpoch [4070/5000], Loss: 0.2859\nEpoch [4080/5000], Loss: 0.2833\nEpoch [4090/5000], Loss: 0.2851\nEpoch [4100/5000], Loss: 0.2842\nEpoch [4110/5000], Loss: 0.2832\nEpoch [4120/5000], Loss: 0.2853\nEpoch [4130/5000], Loss: 0.2846\nEpoch [4140/5000], Loss: 0.2831\nEpoch [4150/5000], Loss: 0.2856\nEpoch [4160/5000], Loss: 0.2830\nEpoch [4170/5000], Loss: 0.2866\nEpoch [4180/5000], Loss: 0.2853\nEpoch [4190/5000], Loss: 0.2831\nEpoch [4200/5000], Loss: 0.2841\nEpoch [4210/5000], Loss: 0.2858\nEpoch [4220/5000], Loss: 0.2851\nEpoch [4230/5000], Loss: 0.2828\nEpoch [4240/5000], Loss: 0.2854\nEpoch [4250/5000], Loss: 0.2833\nEpoch [4260/5000], Loss: 0.2859\nEpoch [4270/5000], Loss: 0.2827\nEpoch [4280/5000], Loss: 0.2859\nEpoch [4290/5000], Loss: 0.2834\nEpoch [4300/5000], Loss: 0.2828\nEpoch [4310/5000], Loss: 0.2845\nEpoch [4320/5000], Loss: 0.2847\nEpoch [4330/5000], Loss: 0.2833\nEpoch [4340/5000], Loss: 0.2846\nEpoch [4350/5000], Loss: 0.2830\nEpoch [4360/5000], Loss: 0.2835\nEpoch [4370/5000], Loss: 0.2845\nEpoch [4380/5000], Loss: 0.2828\nEpoch [4390/5000], Loss: 0.2826\nEpoch [4400/5000], Loss: 0.2855\nEpoch [4410/5000], Loss: 0.2844\nEpoch [4420/5000], Loss: 0.2825\nEpoch [4430/5000], Loss: 0.2844\nEpoch [4440/5000], Loss: 0.2828\nEpoch [4450/5000], Loss: 0.2829\nEpoch [4460/5000], Loss: 0.2850\nEpoch [4470/5000], Loss: 0.2838\nEpoch [4480/5000], Loss: 0.2826\nEpoch [4490/5000], Loss: 0.2831\nEpoch [4500/5000], Loss: 0.2825\nEpoch [4510/5000], Loss: 0.2847\nEpoch [4520/5000], Loss: 0.2821\nEpoch [4530/5000], Loss: 0.2838\nEpoch [4540/5000], Loss: 0.2822\nEpoch [4550/5000], Loss: 0.2851\nEpoch [4560/5000], Loss: 0.2824\nEpoch [4570/5000], Loss: 0.2839\nEpoch [4580/5000], Loss: 0.2828\nEpoch [4590/5000], Loss: 0.2827\nEpoch [4600/5000], Loss: 0.2824\nEpoch [4610/5000], Loss: 0.2838\nEpoch [4620/5000], Loss: 0.2821\nEpoch [4630/5000], Loss: 0.2840\nEpoch [4640/5000], Loss: 0.2821\nEpoch [4650/5000], Loss: 0.2846\nEpoch [4660/5000], Loss: 0.2820\nEpoch [4670/5000], Loss: 0.2835\nEpoch [4680/5000], Loss: 0.2850\nEpoch [4690/5000], Loss: 0.2827\nEpoch [4700/5000], Loss: 0.2819\nEpoch [4710/5000], Loss: 0.2829\nEpoch [4720/5000], Loss: 0.2817\nEpoch [4730/5000], Loss: 0.2840\nEpoch [4740/5000], Loss: 0.2816\nEpoch [4750/5000], Loss: 0.2826\nEpoch [4760/5000], Loss: 0.2835\nEpoch [4770/5000], Loss: 0.2834\nEpoch [4780/5000], Loss: 0.2814\nEpoch [4790/5000], Loss: 0.2829\nEpoch [4800/5000], Loss: 0.2814\nEpoch [4810/5000], Loss: 0.2846\nEpoch [4820/5000], Loss: 0.2852\nEpoch [4830/5000], Loss: 0.2826\nEpoch [4840/5000], Loss: 0.2815\nEpoch [4850/5000], Loss: 0.2836\nEpoch [4860/5000], Loss: 0.2814\nEpoch [4870/5000], Loss: 0.2840\nEpoch [4880/5000], Loss: 0.2842\nEpoch [4890/5000], Loss: 0.2816\nEpoch [4900/5000], Loss: 0.2825\nEpoch [4910/5000], Loss: 0.2819\nEpoch [4920/5000], Loss: 0.2814\nEpoch [4930/5000], Loss: 0.2838\nEpoch [4940/5000], Loss: 0.2820\nEpoch [4950/5000], Loss: 0.2814\nEpoch [4960/5000], Loss: 0.2837\nEpoch [4970/5000], Loss: 0.2813\nEpoch [4980/5000], Loss: 0.2840\nEpoch [4990/5000], Loss: 0.2817\nEpoch [5000/5000], Loss: 0.2830\n","output_type":"stream"}],"execution_count":42},{"cell_type":"markdown","source":"### Evaluating the Model","metadata":{}},{"cell_type":"code","source":"# Set the model to evaluation mode (no gradient updates)\nmodel.eval()\n\nwith torch.no_grad():  # No gradient calculation during inference\n    # Forward pass through the model\n    outputs = model(X_test_tensor)\n\n    # Convert probabilities to binary labels (0 or 1)\n    predicted = (outputs >= 0.5).float()\n\n# Calculate accuracy\n# Move tensors to CPU for use with scikit-learn\ny_pred = predicted.cpu().numpy()  # Convert to NumPy array\ny_test_cpu = Y_test_tensor.cpu().numpy()  # Convert to NumPy array\n\n# Calculate accuracy using scikit-learn's accuracy_score\naccuracy = accuracy_score(y_pred, y_test_cpu)\nprint(f'Accuracy: {accuracy * 100:.2f}%')\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qNRDapuFlUCH","outputId":"c106860a-8290-49a2-b29c-7a6951b9100e","trusted":true,"execution":{"iopub.status.busy":"2024-12-11T09:58:14.156806Z","iopub.execute_input":"2024-12-11T09:58:14.157633Z","iopub.status.idle":"2024-12-11T09:58:14.188806Z","shell.execute_reply.started":"2024-12-11T09:58:14.157590Z","shell.execute_reply":"2024-12-11T09:58:14.187863Z"}},"outputs":[{"name":"stdout","text":"Accuracy: 88.11%\n","output_type":"stream"}],"execution_count":43},{"cell_type":"markdown","source":"### Trying Model 2","metadata":{}},{"cell_type":"code","source":"# let us try another model and see if accuracy improves:\nmodel1=nn.Sequential(\n    # Defining a 4 layer neural network - 3 hidden + 1 output layer. We use ReLU activation function for the hidden layers and Sigmoid activation function for the\n    # output layer since it is a binary classification problem\n    nn.Linear(16,128), # input layer to first hidden layer (16 -> 128 neurons)\n    nn.ReLU(), # Rectified Linear Unit Activation Function\n    nn.Dropout(0.5),\n    nn.Linear(128,64), # first hidden layer to second hidden layer (128 -> 64 neurons)\n    nn.ReLU(), # Rectified Linear Unit Activation Function again\n    nn.Dropout(0.4),\n    nn.Linear(64,32), # second hidden layer to third hidden layer (64 -> 32 neurons)\n    nn.ReLU(), # Rectified Linear Unit Activation Function\n    nn.Dropout(0.2),\n    nn.Linear(32,1), # third hidden layer to output layer (32 -> 1 neuron)\n    nn.Sigmoid() # Sigmoid activation function since it is binary classification\n)","metadata":{"id":"RsPyTMOJltjI","trusted":true,"execution":{"iopub.status.busy":"2024-12-11T10:25:53.910137Z","iopub.execute_input":"2024-12-11T10:25:53.910869Z","iopub.status.idle":"2024-12-11T10:25:53.917329Z","shell.execute_reply.started":"2024-12-11T10:25:53.910834Z","shell.execute_reply":"2024-12-11T10:25:53.916476Z"}},"outputs":[],"execution_count":136},{"cell_type":"code","source":"print(model1)","metadata":{"id":"2TdEKC3CoRTt","colab":{"base_uri":"https://localhost:8080/"},"outputId":"5de7de89-7858-4ff8-8cc0-248e5e61acb2","trusted":true,"execution":{"iopub.status.busy":"2024-12-11T10:22:43.150108Z","iopub.execute_input":"2024-12-11T10:22:43.150847Z","iopub.status.idle":"2024-12-11T10:22:43.155317Z","shell.execute_reply.started":"2024-12-11T10:22:43.150813Z","shell.execute_reply":"2024-12-11T10:22:43.154370Z"}},"outputs":[{"name":"stdout","text":"Sequential(\n  (0): Linear(in_features=16, out_features=128, bias=True)\n  (1): ReLU()\n  (2): Dropout(p=0.5, inplace=False)\n  (3): Linear(in_features=128, out_features=64, bias=True)\n  (4): ReLU()\n  (5): Dropout(p=0.4, inplace=False)\n  (6): Linear(in_features=64, out_features=32, bias=True)\n  (7): ReLU()\n  (8): Dropout(p=0.2, inplace=False)\n  (9): Linear(in_features=32, out_features=1, bias=True)\n  (10): Sigmoid()\n)\n","output_type":"stream"}],"execution_count":122},{"cell_type":"code","source":"model1=model1.to(device)\noptimizer=optim.Adam(model1.parameters(),lr=0.015)","metadata":{"id":"MSOyslG9pxBA","trusted":true,"execution":{"iopub.status.busy":"2024-12-11T10:22:44.086804Z","iopub.execute_input":"2024-12-11T10:22:44.087179Z","iopub.status.idle":"2024-12-11T10:22:44.093519Z","shell.execute_reply.started":"2024-12-11T10:22:44.087141Z","shell.execute_reply":"2024-12-11T10:22:44.092181Z"}},"outputs":[],"execution_count":123},{"cell_type":"code","source":"# train the new model\nnum_epochs = 3000  # Number of epochs for training\n\nfor epoch in range(num_epochs):\n    model1.train()  # Set model to training mode\n\n    # Forward pass\n    outputs = model1(X_train_tensor)  # Get predictions from model\n    loss = loss_function(outputs, Y_train_tensor)  # Calculate loss\n\n    # Backward pass\n    optimizer.zero_grad()  # Clear previous gradients\n    loss.backward()  # Backpropagate gradients\n    optimizer.step()  # Update weights\n\n    # Print loss every 10 epochs\n    if (epoch + 1) % 10 == 0:\n        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aHGRx9VippFH","outputId":"e254bc0f-08e8-492e-ac84-708b6d1c3b54","trusted":true,"execution":{"iopub.status.busy":"2024-12-11T10:22:45.516167Z","iopub.execute_input":"2024-12-11T10:22:45.516828Z","iopub.status.idle":"2024-12-11T10:23:22.788424Z","shell.execute_reply.started":"2024-12-11T10:22:45.516790Z","shell.execute_reply":"2024-12-11T10:23:22.787438Z"}},"outputs":[{"name":"stdout","text":"Epoch [10/3000], Loss: 0.3420\nEpoch [20/3000], Loss: 0.3376\nEpoch [30/3000], Loss: 0.3297\nEpoch [40/3000], Loss: 0.3287\nEpoch [50/3000], Loss: 0.3275\nEpoch [60/3000], Loss: 0.3264\nEpoch [70/3000], Loss: 0.3261\nEpoch [80/3000], Loss: 0.3253\nEpoch [90/3000], Loss: 0.3244\nEpoch [100/3000], Loss: 0.3247\nEpoch [110/3000], Loss: 0.3237\nEpoch [120/3000], Loss: 0.3234\nEpoch [130/3000], Loss: 0.3225\nEpoch [140/3000], Loss: 0.3224\nEpoch [150/3000], Loss: 0.3208\nEpoch [160/3000], Loss: 0.3205\nEpoch [170/3000], Loss: 0.3205\nEpoch [180/3000], Loss: 0.3205\nEpoch [190/3000], Loss: 0.3198\nEpoch [200/3000], Loss: 0.3190\nEpoch [210/3000], Loss: 0.3189\nEpoch [220/3000], Loss: 0.3188\nEpoch [230/3000], Loss: 0.3187\nEpoch [240/3000], Loss: 0.3191\nEpoch [250/3000], Loss: 0.3184\nEpoch [260/3000], Loss: 0.3180\nEpoch [270/3000], Loss: 0.3188\nEpoch [280/3000], Loss: 0.3184\nEpoch [290/3000], Loss: 0.3176\nEpoch [300/3000], Loss: 0.3178\nEpoch [310/3000], Loss: 0.3177\nEpoch [320/3000], Loss: 0.3180\nEpoch [330/3000], Loss: 0.3174\nEpoch [340/3000], Loss: 0.3169\nEpoch [350/3000], Loss: 0.3173\nEpoch [360/3000], Loss: 0.3170\nEpoch [370/3000], Loss: 0.3168\nEpoch [380/3000], Loss: 0.3166\nEpoch [390/3000], Loss: 0.3168\nEpoch [400/3000], Loss: 0.3165\nEpoch [410/3000], Loss: 0.3171\nEpoch [420/3000], Loss: 0.3164\nEpoch [430/3000], Loss: 0.3163\nEpoch [440/3000], Loss: 0.3153\nEpoch [450/3000], Loss: 0.3158\nEpoch [460/3000], Loss: 0.3162\nEpoch [470/3000], Loss: 0.3163\nEpoch [480/3000], Loss: 0.3158\nEpoch [490/3000], Loss: 0.3155\nEpoch [500/3000], Loss: 0.3156\nEpoch [510/3000], Loss: 0.3155\nEpoch [520/3000], Loss: 0.3154\nEpoch [530/3000], Loss: 0.3149\nEpoch [540/3000], Loss: 0.3154\nEpoch [550/3000], Loss: 0.3145\nEpoch [560/3000], Loss: 0.3152\nEpoch [570/3000], Loss: 0.3152\nEpoch [580/3000], Loss: 0.3154\nEpoch [590/3000], Loss: 0.3149\nEpoch [600/3000], Loss: 0.3151\nEpoch [610/3000], Loss: 0.3148\nEpoch [620/3000], Loss: 0.3151\nEpoch [630/3000], Loss: 0.3144\nEpoch [640/3000], Loss: 0.3147\nEpoch [650/3000], Loss: 0.3145\nEpoch [660/3000], Loss: 0.3149\nEpoch [670/3000], Loss: 0.3145\nEpoch [680/3000], Loss: 0.3143\nEpoch [690/3000], Loss: 0.3139\nEpoch [700/3000], Loss: 0.3140\nEpoch [710/3000], Loss: 0.3141\nEpoch [720/3000], Loss: 0.3142\nEpoch [730/3000], Loss: 0.3139\nEpoch [740/3000], Loss: 0.3141\nEpoch [750/3000], Loss: 0.3142\nEpoch [760/3000], Loss: 0.3135\nEpoch [770/3000], Loss: 0.3140\nEpoch [780/3000], Loss: 0.3138\nEpoch [790/3000], Loss: 0.3142\nEpoch [800/3000], Loss: 0.3138\nEpoch [810/3000], Loss: 0.3137\nEpoch [820/3000], Loss: 0.3136\nEpoch [830/3000], Loss: 0.3144\nEpoch [840/3000], Loss: 0.3140\nEpoch [850/3000], Loss: 0.3141\nEpoch [860/3000], Loss: 0.3143\nEpoch [870/3000], Loss: 0.3139\nEpoch [880/3000], Loss: 0.3135\nEpoch [890/3000], Loss: 0.3142\nEpoch [900/3000], Loss: 0.3141\nEpoch [910/3000], Loss: 0.3139\nEpoch [920/3000], Loss: 0.3140\nEpoch [930/3000], Loss: 0.3141\nEpoch [940/3000], Loss: 0.3136\nEpoch [950/3000], Loss: 0.3141\nEpoch [960/3000], Loss: 0.3133\nEpoch [970/3000], Loss: 0.3134\nEpoch [980/3000], Loss: 0.3135\nEpoch [990/3000], Loss: 0.3137\nEpoch [1000/3000], Loss: 0.3134\nEpoch [1010/3000], Loss: 0.3131\nEpoch [1020/3000], Loss: 0.3133\nEpoch [1030/3000], Loss: 0.3135\nEpoch [1040/3000], Loss: 0.3130\nEpoch [1050/3000], Loss: 0.3136\nEpoch [1060/3000], Loss: 0.3134\nEpoch [1070/3000], Loss: 0.3128\nEpoch [1080/3000], Loss: 0.3130\nEpoch [1090/3000], Loss: 0.3132\nEpoch [1100/3000], Loss: 0.3131\nEpoch [1110/3000], Loss: 0.3132\nEpoch [1120/3000], Loss: 0.3134\nEpoch [1130/3000], Loss: 0.3132\nEpoch [1140/3000], Loss: 0.3132\nEpoch [1150/3000], Loss: 0.3131\nEpoch [1160/3000], Loss: 0.3137\nEpoch [1170/3000], Loss: 0.3130\nEpoch [1180/3000], Loss: 0.3131\nEpoch [1190/3000], Loss: 0.3132\nEpoch [1200/3000], Loss: 0.3128\nEpoch [1210/3000], Loss: 0.3128\nEpoch [1220/3000], Loss: 0.3128\nEpoch [1230/3000], Loss: 0.3124\nEpoch [1240/3000], Loss: 0.3128\nEpoch [1250/3000], Loss: 0.3127\nEpoch [1260/3000], Loss: 0.3125\nEpoch [1270/3000], Loss: 0.3134\nEpoch [1280/3000], Loss: 0.3122\nEpoch [1290/3000], Loss: 0.3125\nEpoch [1300/3000], Loss: 0.3130\nEpoch [1310/3000], Loss: 0.3130\nEpoch [1320/3000], Loss: 0.3126\nEpoch [1330/3000], Loss: 0.3126\nEpoch [1340/3000], Loss: 0.3123\nEpoch [1350/3000], Loss: 0.3120\nEpoch [1360/3000], Loss: 0.3124\nEpoch [1370/3000], Loss: 0.3124\nEpoch [1380/3000], Loss: 0.3125\nEpoch [1390/3000], Loss: 0.3130\nEpoch [1400/3000], Loss: 0.3124\nEpoch [1410/3000], Loss: 0.3129\nEpoch [1420/3000], Loss: 0.3125\nEpoch [1430/3000], Loss: 0.3120\nEpoch [1440/3000], Loss: 0.3128\nEpoch [1450/3000], Loss: 0.3125\nEpoch [1460/3000], Loss: 0.3127\nEpoch [1470/3000], Loss: 0.3126\nEpoch [1480/3000], Loss: 0.3124\nEpoch [1490/3000], Loss: 0.3124\nEpoch [1500/3000], Loss: 0.3122\nEpoch [1510/3000], Loss: 0.3128\nEpoch [1520/3000], Loss: 0.3126\nEpoch [1530/3000], Loss: 0.3122\nEpoch [1540/3000], Loss: 0.3120\nEpoch [1550/3000], Loss: 0.3129\nEpoch [1560/3000], Loss: 0.3124\nEpoch [1570/3000], Loss: 0.3123\nEpoch [1580/3000], Loss: 0.3126\nEpoch [1590/3000], Loss: 0.3123\nEpoch [1600/3000], Loss: 0.3125\nEpoch [1610/3000], Loss: 0.3122\nEpoch [1620/3000], Loss: 0.3120\nEpoch [1630/3000], Loss: 0.3121\nEpoch [1640/3000], Loss: 0.3119\nEpoch [1650/3000], Loss: 0.3123\nEpoch [1660/3000], Loss: 0.3123\nEpoch [1670/3000], Loss: 0.3120\nEpoch [1680/3000], Loss: 0.3118\nEpoch [1690/3000], Loss: 0.3124\nEpoch [1700/3000], Loss: 0.3120\nEpoch [1710/3000], Loss: 0.3117\nEpoch [1720/3000], Loss: 0.3121\nEpoch [1730/3000], Loss: 0.3119\nEpoch [1740/3000], Loss: 0.3121\nEpoch [1750/3000], Loss: 0.3124\nEpoch [1760/3000], Loss: 0.3122\nEpoch [1770/3000], Loss: 0.3121\nEpoch [1780/3000], Loss: 0.3123\nEpoch [1790/3000], Loss: 0.3119\nEpoch [1800/3000], Loss: 0.3118\nEpoch [1810/3000], Loss: 0.3118\nEpoch [1820/3000], Loss: 0.3127\nEpoch [1830/3000], Loss: 0.3124\nEpoch [1840/3000], Loss: 0.3128\nEpoch [1850/3000], Loss: 0.3118\nEpoch [1860/3000], Loss: 0.3119\nEpoch [1870/3000], Loss: 0.3122\nEpoch [1880/3000], Loss: 0.3122\nEpoch [1890/3000], Loss: 0.3121\nEpoch [1900/3000], Loss: 0.3123\nEpoch [1910/3000], Loss: 0.3122\nEpoch [1920/3000], Loss: 0.3120\nEpoch [1930/3000], Loss: 0.3121\nEpoch [1940/3000], Loss: 0.3123\nEpoch [1950/3000], Loss: 0.3119\nEpoch [1960/3000], Loss: 0.3122\nEpoch [1970/3000], Loss: 0.3115\nEpoch [1980/3000], Loss: 0.3120\nEpoch [1990/3000], Loss: 0.3120\nEpoch [2000/3000], Loss: 0.3124\nEpoch [2010/3000], Loss: 0.3117\nEpoch [2020/3000], Loss: 0.3120\nEpoch [2030/3000], Loss: 0.3119\nEpoch [2040/3000], Loss: 0.3120\nEpoch [2050/3000], Loss: 0.3119\nEpoch [2060/3000], Loss: 0.3117\nEpoch [2070/3000], Loss: 0.3117\nEpoch [2080/3000], Loss: 0.3119\nEpoch [2090/3000], Loss: 0.3118\nEpoch [2100/3000], Loss: 0.3118\nEpoch [2110/3000], Loss: 0.3122\nEpoch [2120/3000], Loss: 0.3118\nEpoch [2130/3000], Loss: 0.3117\nEpoch [2140/3000], Loss: 0.3120\nEpoch [2150/3000], Loss: 0.3122\nEpoch [2160/3000], Loss: 0.3118\nEpoch [2170/3000], Loss: 0.3119\nEpoch [2180/3000], Loss: 0.3117\nEpoch [2190/3000], Loss: 0.3119\nEpoch [2200/3000], Loss: 0.3122\nEpoch [2210/3000], Loss: 0.3116\nEpoch [2220/3000], Loss: 0.3118\nEpoch [2230/3000], Loss: 0.3118\nEpoch [2240/3000], Loss: 0.3115\nEpoch [2250/3000], Loss: 0.3120\nEpoch [2260/3000], Loss: 0.3121\nEpoch [2270/3000], Loss: 0.3120\nEpoch [2280/3000], Loss: 0.3118\nEpoch [2290/3000], Loss: 0.3120\nEpoch [2300/3000], Loss: 0.3123\nEpoch [2310/3000], Loss: 0.3115\nEpoch [2320/3000], Loss: 0.3116\nEpoch [2330/3000], Loss: 0.3118\nEpoch [2340/3000], Loss: 0.3118\nEpoch [2350/3000], Loss: 0.3116\nEpoch [2360/3000], Loss: 0.3116\nEpoch [2370/3000], Loss: 0.3116\nEpoch [2380/3000], Loss: 0.3120\nEpoch [2390/3000], Loss: 0.3119\nEpoch [2400/3000], Loss: 0.3117\nEpoch [2410/3000], Loss: 0.3122\nEpoch [2420/3000], Loss: 0.3117\nEpoch [2430/3000], Loss: 0.3114\nEpoch [2440/3000], Loss: 0.3116\nEpoch [2450/3000], Loss: 0.3115\nEpoch [2460/3000], Loss: 0.3121\nEpoch [2470/3000], Loss: 0.3117\nEpoch [2480/3000], Loss: 0.3117\nEpoch [2490/3000], Loss: 0.3117\nEpoch [2500/3000], Loss: 0.3112\nEpoch [2510/3000], Loss: 0.3118\nEpoch [2520/3000], Loss: 0.3115\nEpoch [2530/3000], Loss: 0.3114\nEpoch [2540/3000], Loss: 0.3118\nEpoch [2550/3000], Loss: 0.3114\nEpoch [2560/3000], Loss: 0.3115\nEpoch [2570/3000], Loss: 0.3121\nEpoch [2580/3000], Loss: 0.3117\nEpoch [2590/3000], Loss: 0.3113\nEpoch [2600/3000], Loss: 0.3118\nEpoch [2610/3000], Loss: 0.3115\nEpoch [2620/3000], Loss: 0.3118\nEpoch [2630/3000], Loss: 0.3115\nEpoch [2640/3000], Loss: 0.3121\nEpoch [2650/3000], Loss: 0.3119\nEpoch [2660/3000], Loss: 0.3113\nEpoch [2670/3000], Loss: 0.3117\nEpoch [2680/3000], Loss: 0.3113\nEpoch [2690/3000], Loss: 0.3117\nEpoch [2700/3000], Loss: 0.3116\nEpoch [2710/3000], Loss: 0.3114\nEpoch [2720/3000], Loss: 0.3114\nEpoch [2730/3000], Loss: 0.3120\nEpoch [2740/3000], Loss: 0.3115\nEpoch [2750/3000], Loss: 0.3118\nEpoch [2760/3000], Loss: 0.3118\nEpoch [2770/3000], Loss: 0.3116\nEpoch [2780/3000], Loss: 0.3123\nEpoch [2790/3000], Loss: 0.3117\nEpoch [2800/3000], Loss: 0.3116\nEpoch [2810/3000], Loss: 0.3112\nEpoch [2820/3000], Loss: 0.3117\nEpoch [2830/3000], Loss: 0.3117\nEpoch [2840/3000], Loss: 0.3114\nEpoch [2850/3000], Loss: 0.3110\nEpoch [2860/3000], Loss: 0.3114\nEpoch [2870/3000], Loss: 0.3111\nEpoch [2880/3000], Loss: 0.3112\nEpoch [2890/3000], Loss: 0.3114\nEpoch [2900/3000], Loss: 0.3115\nEpoch [2910/3000], Loss: 0.3113\nEpoch [2920/3000], Loss: 0.3112\nEpoch [2930/3000], Loss: 0.3110\nEpoch [2940/3000], Loss: 0.3113\nEpoch [2950/3000], Loss: 0.3113\nEpoch [2960/3000], Loss: 0.3113\nEpoch [2970/3000], Loss: 0.3115\nEpoch [2980/3000], Loss: 0.3112\nEpoch [2990/3000], Loss: 0.3112\nEpoch [3000/3000], Loss: 0.3115\n","output_type":"stream"}],"execution_count":124},{"cell_type":"code","source":"# Set the model to evaluation mode (no gradient updates)\nmodel1.eval()\n\nwith torch.no_grad():  # No gradient calculation during inference\n    # Forward pass through the model\n    outputs = model1(X_test_tensor)\n\n    # Convert probabilities to binary labels (0 or 1)\n    predicted = (outputs >= 0.5).float()\n\n# Calculate accuracy\n# Move tensors to CPU for use with scikit-learn\ny_pred = predicted.cpu().numpy()  # Convert to NumPy array\ny_test_cpu = Y_test_tensor.cpu().numpy()  # Convert to NumPy array\n\n# Calculate accuracy using scikit-learn's accuracy_score\naccuracy = accuracy_score(y_pred, y_test_cpu)\nprint(f'Accuracy: {accuracy * 100:.2f}%')\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"owG2FhTpp0F5","outputId":"2064fd59-27e9-45d4-faa8-d512d1dde177","trusted":true,"execution":{"iopub.status.busy":"2024-12-11T10:23:57.614699Z","iopub.execute_input":"2024-12-11T10:23:57.615721Z","iopub.status.idle":"2024-12-11T10:23:57.628658Z","shell.execute_reply.started":"2024-12-11T10:23:57.615682Z","shell.execute_reply":"2024-12-11T10:23:57.627777Z"}},"outputs":[{"name":"stdout","text":"Accuracy: 88.46%\n","output_type":"stream"}],"execution_count":125},{"cell_type":"markdown","source":"### Making predictions with the second model","metadata":{"id":"Nqj9e5iiqSmO"}},{"cell_type":"code","source":"new_X_test=pd.read_csv('/kaggle/input/ml-assignment/Processed_test.csv')","metadata":{"id":"xidvnDLMreEN","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T07:23:25.074617Z","iopub.execute_input":"2024-12-12T07:23:25.074955Z","iopub.status.idle":"2024-12-12T07:23:25.244027Z","shell.execute_reply.started":"2024-12-12T07:23:25.074926Z","shell.execute_reply":"2024-12-12T07:23:25.243317Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"print(new_X_test.shape)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B1CagnyNszej","outputId":"a88b1033-e7c2-42ac-d2e2-21cb3229e096","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T07:23:27.140699Z","iopub.execute_input":"2024-12-12T07:23:27.141396Z","iopub.status.idle":"2024-12-12T07:23:27.145636Z","shell.execute_reply.started":"2024-12-12T07:23:27.141352Z","shell.execute_reply":"2024-12-12T07:23:27.144837Z"}},"outputs":[{"name":"stdout","text":"(51070, 17)\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"new_X_test.head()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":226},"id":"JA7qE6A4s1oV","outputId":"afc9338a-f633-4058-9751-e0e3418030d4","trusted":true,"execution":{"iopub.status.busy":"2024-12-11T05:51:37.835591Z","iopub.execute_input":"2024-12-11T05:51:37.836222Z","iopub.status.idle":"2024-12-11T05:51:37.849575Z","shell.execute_reply.started":"2024-12-11T05:51:37.836187Z","shell.execute_reply":"2024-12-11T05:51:37.848849Z"}},"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"       LoanID  Age  Income  LoanAmount  CreditScore  MonthsEmployed  \\\n0  CKV34LU7V7   55  112656       92393          581             113   \n1  62KTYNH93J   56   91569      131575          641              54   \n2  JGFUSOIUH7   26   78169       75417          569             105   \n3  4538THBHOX   26   63033       10804          326             118   \n4  DXLNA06JHR   24   29665       21182          662             102   \n\n   NumCreditLines  InterestRate  LoanTerm  DTIRatio  Education  \\\n0               2         23.54        36      0.15          4   \n1               1         15.19        12      0.43          1   \n2               3         18.02        12      0.29          3   \n3               1         14.71        24      0.41          1   \n4               3         15.02        60      0.69          4   \n\n   EmploymentType  MaritalStatus  HasMortgage  HasDependents  LoanPurpose  \\\n0               3              2            1              1            4   \n1               2              1            1              1            1   \n2               2              3            1              1            1   \n3               2              2            0              0            0   \n4               1              2            0              1            0   \n\n   HasCoSigner  \n0            0  \n1            1  \n2            1  \n3            1  \n4            1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>LoanID</th>\n      <th>Age</th>\n      <th>Income</th>\n      <th>LoanAmount</th>\n      <th>CreditScore</th>\n      <th>MonthsEmployed</th>\n      <th>NumCreditLines</th>\n      <th>InterestRate</th>\n      <th>LoanTerm</th>\n      <th>DTIRatio</th>\n      <th>Education</th>\n      <th>EmploymentType</th>\n      <th>MaritalStatus</th>\n      <th>HasMortgage</th>\n      <th>HasDependents</th>\n      <th>LoanPurpose</th>\n      <th>HasCoSigner</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>CKV34LU7V7</td>\n      <td>55</td>\n      <td>112656</td>\n      <td>92393</td>\n      <td>581</td>\n      <td>113</td>\n      <td>2</td>\n      <td>23.54</td>\n      <td>36</td>\n      <td>0.15</td>\n      <td>4</td>\n      <td>3</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>4</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>62KTYNH93J</td>\n      <td>56</td>\n      <td>91569</td>\n      <td>131575</td>\n      <td>641</td>\n      <td>54</td>\n      <td>1</td>\n      <td>15.19</td>\n      <td>12</td>\n      <td>0.43</td>\n      <td>1</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>JGFUSOIUH7</td>\n      <td>26</td>\n      <td>78169</td>\n      <td>75417</td>\n      <td>569</td>\n      <td>105</td>\n      <td>3</td>\n      <td>18.02</td>\n      <td>12</td>\n      <td>0.29</td>\n      <td>3</td>\n      <td>2</td>\n      <td>3</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4538THBHOX</td>\n      <td>26</td>\n      <td>63033</td>\n      <td>10804</td>\n      <td>326</td>\n      <td>118</td>\n      <td>1</td>\n      <td>14.71</td>\n      <td>24</td>\n      <td>0.41</td>\n      <td>1</td>\n      <td>2</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>DXLNA06JHR</td>\n      <td>24</td>\n      <td>29665</td>\n      <td>21182</td>\n      <td>662</td>\n      <td>102</td>\n      <td>3</td>\n      <td>15.02</td>\n      <td>60</td>\n      <td>0.69</td>\n      <td>4</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":36},{"cell_type":"code","source":"# need to scale it first","metadata":{"id":"26iPDtb5tSyl","trusted":true,"execution":{"iopub.status.busy":"2024-12-11T05:51:40.100992Z","iopub.execute_input":"2024-12-11T05:51:40.101585Z","iopub.status.idle":"2024-12-11T05:51:40.105550Z","shell.execute_reply.started":"2024-12-11T05:51:40.101548Z","shell.execute_reply":"2024-12-11T05:51:40.104688Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"ids=new_X_test['LoanID']\nnew_X_test=new_X_test.drop(columns=['LoanID'])","metadata":{"id":"gVJiuwZhtanh","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T07:23:30.443279Z","iopub.execute_input":"2024-12-12T07:23:30.443604Z","iopub.status.idle":"2024-12-12T07:23:30.449630Z","shell.execute_reply.started":"2024-12-12T07:23:30.443578Z","shell.execute_reply":"2024-12-12T07:23:30.448596Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"new_X_test.head()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"g2RdhET_uJlz","outputId":"b71a2dfb-e54a-468e-86c7-2921829c89f1","trusted":true,"execution":{"iopub.status.busy":"2024-12-11T07:57:57.238871Z","iopub.execute_input":"2024-12-11T07:57:57.239227Z","iopub.status.idle":"2024-12-11T07:57:57.256754Z","shell.execute_reply.started":"2024-12-11T07:57:57.239194Z","shell.execute_reply":"2024-12-11T07:57:57.256030Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"   Age  Income  LoanAmount  CreditScore  MonthsEmployed  NumCreditLines  \\\n0   55  112656       92393          581             113               2   \n1   56   91569      131575          641              54               1   \n2   26   78169       75417          569             105               3   \n3   26   63033       10804          326             118               1   \n4   24   29665       21182          662             102               3   \n\n   InterestRate  LoanTerm  DTIRatio  Education  EmploymentType  MaritalStatus  \\\n0         23.54        36      0.15          4               3              2   \n1         15.19        12      0.43          1               2              1   \n2         18.02        12      0.29          3               2              3   \n3         14.71        24      0.41          1               2              2   \n4         15.02        60      0.69          4               1              2   \n\n   HasMortgage  HasDependents  LoanPurpose  HasCoSigner  \n0            1              1            4            0  \n1            1              1            1            1  \n2            1              1            1            1  \n3            0              0            0            1  \n4            0              1            0            1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Age</th>\n      <th>Income</th>\n      <th>LoanAmount</th>\n      <th>CreditScore</th>\n      <th>MonthsEmployed</th>\n      <th>NumCreditLines</th>\n      <th>InterestRate</th>\n      <th>LoanTerm</th>\n      <th>DTIRatio</th>\n      <th>Education</th>\n      <th>EmploymentType</th>\n      <th>MaritalStatus</th>\n      <th>HasMortgage</th>\n      <th>HasDependents</th>\n      <th>LoanPurpose</th>\n      <th>HasCoSigner</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>55</td>\n      <td>112656</td>\n      <td>92393</td>\n      <td>581</td>\n      <td>113</td>\n      <td>2</td>\n      <td>23.54</td>\n      <td>36</td>\n      <td>0.15</td>\n      <td>4</td>\n      <td>3</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>4</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>56</td>\n      <td>91569</td>\n      <td>131575</td>\n      <td>641</td>\n      <td>54</td>\n      <td>1</td>\n      <td>15.19</td>\n      <td>12</td>\n      <td>0.43</td>\n      <td>1</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>26</td>\n      <td>78169</td>\n      <td>75417</td>\n      <td>569</td>\n      <td>105</td>\n      <td>3</td>\n      <td>18.02</td>\n      <td>12</td>\n      <td>0.29</td>\n      <td>3</td>\n      <td>2</td>\n      <td>3</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>26</td>\n      <td>63033</td>\n      <td>10804</td>\n      <td>326</td>\n      <td>118</td>\n      <td>1</td>\n      <td>14.71</td>\n      <td>24</td>\n      <td>0.41</td>\n      <td>1</td>\n      <td>2</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>24</td>\n      <td>29665</td>\n      <td>21182</td>\n      <td>662</td>\n      <td>102</td>\n      <td>3</td>\n      <td>15.02</td>\n      <td>60</td>\n      <td>0.69</td>\n      <td>4</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"scaled_new_X_test=scaler.transform(new_X_test)","metadata":{"id":"-B4-_fCZtezE","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T07:23:33.699771Z","iopub.execute_input":"2024-12-12T07:23:33.700110Z","iopub.status.idle":"2024-12-12T07:23:33.710281Z","shell.execute_reply.started":"2024-12-12T07:23:33.700080Z","shell.execute_reply":"2024-12-12T07:23:33.709417Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"# Convert it to tensor\nnew_x_test_tensor=torch.tensor(scaled_new_X_test,dtype=torch.float32)\n# move it to gpu\nnew_x_test_tensor=new_x_test_tensor.to(device)","metadata":{"id":"9v63FKwetxH0","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T07:24:18.027126Z","iopub.execute_input":"2024-12-12T07:24:18.027508Z","iopub.status.idle":"2024-12-12T07:24:18.033878Z","shell.execute_reply.started":"2024-12-12T07:24:18.027475Z","shell.execute_reply":"2024-12-12T07:24:18.032843Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"# Set the model to evaluation mode (no gradient updates)\nmodel1.eval()\n\nwith torch.no_grad():  # No gradient calculation during inference\n    # Forward pass through the model\n    outputs = model1(new_x_test_tensor)\n\n    # Convert probabilities to binary labels (0 or 1)\n    predicted = (outputs >= 0.5).float()\n\n# Calculate accuracy\n# Move tensors to CPU for use with scikit-learn\ny_pred = predicted.cpu().numpy()  # Convert to NumPy array","metadata":{"id":"WZhXuTJbt_UK","trusted":true,"execution":{"iopub.status.busy":"2024-12-11T10:24:02.647198Z","iopub.execute_input":"2024-12-11T10:24:02.647544Z","iopub.status.idle":"2024-12-11T10:24:02.654351Z","shell.execute_reply.started":"2024-12-11T10:24:02.647512Z","shell.execute_reply":"2024-12-11T10:24:02.653380Z"}},"outputs":[],"execution_count":126},{"cell_type":"code","source":"y_pred=pd.DataFrame(y_pred)","metadata":{"id":"KdsngvxuuqPj","trusted":true,"execution":{"iopub.status.busy":"2024-12-11T10:24:05.010319Z","iopub.execute_input":"2024-12-11T10:24:05.010699Z","iopub.status.idle":"2024-12-11T10:24:05.015441Z","shell.execute_reply.started":"2024-12-11T10:24:05.010667Z","shell.execute_reply":"2024-12-11T10:24:05.014405Z"}},"outputs":[],"execution_count":127},{"cell_type":"code","source":"y_pred.value_counts()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":178},"id":"Fp4PFxGau2PW","outputId":"52d6be87-967e-4a74-a953-d267d9a92ce4","trusted":true,"execution":{"iopub.status.busy":"2024-12-11T10:24:06.626037Z","iopub.execute_input":"2024-12-11T10:24:06.626338Z","iopub.status.idle":"2024-12-11T10:24:06.636421Z","shell.execute_reply.started":"2024-12-11T10:24:06.626309Z","shell.execute_reply":"2024-12-11T10:24:06.635603Z"}},"outputs":[{"execution_count":128,"output_type":"execute_result","data":{"text/plain":"0  \n0.0    50352\n1.0      718\nName: count, dtype: int64"},"metadata":{}}],"execution_count":128},{"cell_type":"code","source":"print(type(ids))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oLxrtMjnvAL4","outputId":"740e69bf-d59f-42ce-f1ac-c21095f01305","trusted":true,"execution":{"iopub.status.busy":"2024-12-11T10:02:33.241671Z","iopub.execute_input":"2024-12-11T10:02:33.241981Z","iopub.status.idle":"2024-12-11T10:02:33.246851Z","shell.execute_reply.started":"2024-12-11T10:02:33.241952Z","shell.execute_reply":"2024-12-11T10:02:33.245855Z"}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.series.Series'>\n","output_type":"stream"}],"execution_count":71},{"cell_type":"code","source":"print(type(y_pred))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4A_kkeXPvKTw","outputId":"aa4ca57a-5494-47fb-ad49-77394cef3379","trusted":true,"execution":{"iopub.status.busy":"2024-12-11T10:02:33.248009Z","iopub.execute_input":"2024-12-11T10:02:33.248277Z","iopub.status.idle":"2024-12-11T10:02:33.254067Z","shell.execute_reply.started":"2024-12-11T10:02:33.248251Z","shell.execute_reply":"2024-12-11T10:02:33.252954Z"}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\n","output_type":"stream"}],"execution_count":72},{"cell_type":"code","source":"# Combine them into a single DataFrame\nmerged_df = pd.concat([ids, y_pred], axis=1)","metadata":{"id":"_dpuZwDTvOqe","trusted":true,"execution":{"iopub.status.busy":"2024-12-11T10:24:11.755462Z","iopub.execute_input":"2024-12-11T10:24:11.756660Z","iopub.status.idle":"2024-12-11T10:24:11.762830Z","shell.execute_reply.started":"2024-12-11T10:24:11.756610Z","shell.execute_reply":"2024-12-11T10:24:11.761844Z"}},"outputs":[],"execution_count":129},{"cell_type":"code","source":"merged_df.head()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"SsNeOjbHwMR5","outputId":"a5d2ab95-d69a-4e44-ea85-a40cb535746e","trusted":true,"execution":{"iopub.status.busy":"2024-12-11T10:24:13.629053Z","iopub.execute_input":"2024-12-11T10:24:13.629379Z","iopub.status.idle":"2024-12-11T10:24:13.638294Z","shell.execute_reply.started":"2024-12-11T10:24:13.629348Z","shell.execute_reply":"2024-12-11T10:24:13.637364Z"}},"outputs":[{"execution_count":130,"output_type":"execute_result","data":{"text/plain":"       LoanID    0\n0  CKV34LU7V7  0.0\n1  62KTYNH93J  0.0\n2  JGFUSOIUH7  0.0\n3  4538THBHOX  0.0\n4  DXLNA06JHR  0.0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>LoanID</th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>CKV34LU7V7</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>62KTYNH93J</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>JGFUSOIUH7</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4538THBHOX</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>DXLNA06JHR</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":130},{"cell_type":"code","source":"merged_df = merged_df.rename(columns={0: 'Default'})","metadata":{"id":"KOwXmSNnwNyq","trusted":true,"execution":{"iopub.status.busy":"2024-12-11T10:24:15.545342Z","iopub.execute_input":"2024-12-11T10:24:15.545698Z","iopub.status.idle":"2024-12-11T10:24:15.551354Z","shell.execute_reply.started":"2024-12-11T10:24:15.545669Z","shell.execute_reply":"2024-12-11T10:24:15.550461Z"}},"outputs":[],"execution_count":131},{"cell_type":"code","source":"merged_df.head()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"OORkx2SLw52z","outputId":"dc4d3d0d-89a3-4af8-f100-a3a674c3b38e","trusted":true,"execution":{"iopub.status.busy":"2024-12-11T10:24:16.661524Z","iopub.execute_input":"2024-12-11T10:24:16.662321Z","iopub.status.idle":"2024-12-11T10:24:16.671101Z","shell.execute_reply.started":"2024-12-11T10:24:16.662279Z","shell.execute_reply":"2024-12-11T10:24:16.670098Z"}},"outputs":[{"execution_count":132,"output_type":"execute_result","data":{"text/plain":"       LoanID  Default\n0  CKV34LU7V7      0.0\n1  62KTYNH93J      0.0\n2  JGFUSOIUH7      0.0\n3  4538THBHOX      0.0\n4  DXLNA06JHR      0.0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>LoanID</th>\n      <th>Default</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>CKV34LU7V7</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>62KTYNH93J</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>JGFUSOIUH7</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4538THBHOX</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>DXLNA06JHR</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":132},{"cell_type":"code","source":"merged_df['Default']=merged_df['Default'].astype(int)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T10:24:18.478155Z","iopub.execute_input":"2024-12-11T10:24:18.478903Z","iopub.status.idle":"2024-12-11T10:24:18.483807Z","shell.execute_reply.started":"2024-12-11T10:24:18.478864Z","shell.execute_reply":"2024-12-11T10:24:18.482855Z"}},"outputs":[],"execution_count":133},{"cell_type":"code","source":"merged_df.to_csv('predictions.csv',index=False)","metadata":{"id":"jKVySv8Zw7Zg","trusted":true,"execution":{"iopub.status.busy":"2024-12-11T10:24:20.794804Z","iopub.execute_input":"2024-12-11T10:24:20.795530Z","iopub.status.idle":"2024-12-11T10:24:20.834058Z","shell.execute_reply.started":"2024-12-11T10:24:20.795497Z","shell.execute_reply":"2024-12-11T10:24:20.833183Z"}},"outputs":[],"execution_count":134},{"cell_type":"code","source":"# this on submission gave 88.576 % accuracy","metadata":{"id":"ZJYeKXHSxAQK","trusted":true,"execution":{"iopub.status.busy":"2024-12-11T10:25:08.465089Z","iopub.execute_input":"2024-12-11T10:25:08.465481Z","iopub.status.idle":"2024-12-11T10:25:08.469595Z","shell.execute_reply.started":"2024-12-11T10:25:08.465440Z","shell.execute_reply":"2024-12-11T10:25:08.468610Z"}},"outputs":[],"execution_count":135},{"cell_type":"markdown","source":"### Trying a third model to improve accuracy","metadata":{}},{"cell_type":"code","source":"# let us try a third model and see if the accuracy improves\nmodel2=nn.Sequential(\n    # Defining a 4 layer neural network - 3 hidden + 1 output layer. We use ReLU activation function for the hidden layers and Sigmoid activation function for the\n    # output layer since it is a binary classification problem\n    nn.Linear(16,128), # input layer to first hidden layer (16 -> 128 neurons)\n    nn.BatchNorm1d(128),  # Batch Normalization for 128 neurons\n    nn.ReLU(), # Rectified Linear Unit Activation Function\n    nn.Dropout(0.5),\n    nn.Linear(128,64), # first hidden layer to second hidden layer (128 -> 64 neurons)\n    nn.BatchNorm1d(64),  # Batch Normalization for 64 neurons\n    nn.ReLU(), # Rectified Linear Unit Activation Function again\n    nn.Dropout(0.2),\n    nn.Linear(64,32), # second hidden layer to third hidden layer (64 -> 32 neurons)\n    nn.BatchNorm1d(32),  # Batch Normalization for 32 neurons\n    nn.ReLU(), # Rectified Linear Unit Activation Function\n    nn.Dropout(0.1),\n    nn.Linear(32,1), # third hidden layer to output layer (32 -> 1 neuron)\n    nn.Sigmoid() # Sigmoid activation function since it is binary classification\n)","metadata":{"id":"kG2qXel-0Uyd","trusted":true,"execution":{"iopub.status.busy":"2024-12-11T06:22:33.288375Z","iopub.execute_input":"2024-12-11T06:22:33.288674Z","iopub.status.idle":"2024-12-11T06:22:33.295730Z","shell.execute_reply.started":"2024-12-11T06:22:33.288648Z","shell.execute_reply":"2024-12-11T06:22:33.295016Z"}},"outputs":[],"execution_count":88},{"cell_type":"code","source":"model2=model2.to(device)\noptimizer=optim.Adam(model2.parameters(),lr=0.01)","metadata":{"id":"oTT5gGmR0ho9","trusted":true,"execution":{"iopub.status.busy":"2024-12-11T06:22:36.229534Z","iopub.execute_input":"2024-12-11T06:22:36.229826Z","iopub.status.idle":"2024-12-11T06:22:36.235350Z","shell.execute_reply.started":"2024-12-11T06:22:36.229799Z","shell.execute_reply":"2024-12-11T06:22:36.234472Z"}},"outputs":[],"execution_count":89},{"cell_type":"code","source":"# train the new model\nnum_epochs = 3000  # Number of epochs for training\n\nfor epoch in range(num_epochs):\n    model2.train()  # Set model to training mode\n\n    # Forward pass\n    outputs = model2(X_train_tensor)  # Get predictions from model\n    loss = loss_function(outputs, Y_train_tensor)  # Calculate loss\n\n    # Backward pass\n    optimizer.zero_grad()  # Clear previous gradients\n    loss.backward()  # Backpropagate gradients\n    optimizer.step()  # Update weights\n\n    # Print loss every 10 epochs\n    if (epoch + 1) % 10 == 0:\n        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9mPfGwym0tdo","outputId":"629a2d99-98e8-442f-b2af-0ec20545b7c5","trusted":true,"execution":{"iopub.status.busy":"2024-12-11T06:22:37.407583Z","iopub.execute_input":"2024-12-11T06:22:37.407928Z","iopub.status.idle":"2024-12-11T06:23:34.107140Z","shell.execute_reply.started":"2024-12-11T06:22:37.407871Z","shell.execute_reply":"2024-12-11T06:23:34.106279Z"}},"outputs":[{"name":"stdout","text":"Epoch [10/3000], Loss: 0.3334\nEpoch [20/3000], Loss: 0.3264\nEpoch [30/3000], Loss: 0.3232\nEpoch [40/3000], Loss: 0.3186\nEpoch [50/3000], Loss: 0.3179\nEpoch [60/3000], Loss: 0.3167\nEpoch [70/3000], Loss: 0.3162\nEpoch [80/3000], Loss: 0.3159\nEpoch [90/3000], Loss: 0.3150\nEpoch [100/3000], Loss: 0.3143\nEpoch [110/3000], Loss: 0.3140\nEpoch [120/3000], Loss: 0.3136\nEpoch [130/3000], Loss: 0.3134\nEpoch [140/3000], Loss: 0.3132\nEpoch [150/3000], Loss: 0.3130\nEpoch [160/3000], Loss: 0.3125\nEpoch [170/3000], Loss: 0.3123\nEpoch [180/3000], Loss: 0.3126\nEpoch [190/3000], Loss: 0.3121\nEpoch [200/3000], Loss: 0.3119\nEpoch [210/3000], Loss: 0.3118\nEpoch [220/3000], Loss: 0.3116\nEpoch [230/3000], Loss: 0.3116\nEpoch [240/3000], Loss: 0.3113\nEpoch [250/3000], Loss: 0.3109\nEpoch [260/3000], Loss: 0.3111\nEpoch [270/3000], Loss: 0.3111\nEpoch [280/3000], Loss: 0.3109\nEpoch [290/3000], Loss: 0.3104\nEpoch [300/3000], Loss: 0.3105\nEpoch [310/3000], Loss: 0.3106\nEpoch [320/3000], Loss: 0.3106\nEpoch [330/3000], Loss: 0.3103\nEpoch [340/3000], Loss: 0.3099\nEpoch [350/3000], Loss: 0.3098\nEpoch [360/3000], Loss: 0.3097\nEpoch [370/3000], Loss: 0.3093\nEpoch [380/3000], Loss: 0.3094\nEpoch [390/3000], Loss: 0.3094\nEpoch [400/3000], Loss: 0.3090\nEpoch [410/3000], Loss: 0.3092\nEpoch [420/3000], Loss: 0.3090\nEpoch [430/3000], Loss: 0.3093\nEpoch [440/3000], Loss: 0.3087\nEpoch [450/3000], Loss: 0.3086\nEpoch [460/3000], Loss: 0.3090\nEpoch [470/3000], Loss: 0.3085\nEpoch [480/3000], Loss: 0.3082\nEpoch [490/3000], Loss: 0.3083\nEpoch [500/3000], Loss: 0.3084\nEpoch [510/3000], Loss: 0.3084\nEpoch [520/3000], Loss: 0.3086\nEpoch [530/3000], Loss: 0.3086\nEpoch [540/3000], Loss: 0.3083\nEpoch [550/3000], Loss: 0.3079\nEpoch [560/3000], Loss: 0.3079\nEpoch [570/3000], Loss: 0.3076\nEpoch [580/3000], Loss: 0.3083\nEpoch [590/3000], Loss: 0.3079\nEpoch [600/3000], Loss: 0.3076\nEpoch [610/3000], Loss: 0.3078\nEpoch [620/3000], Loss: 0.3076\nEpoch [630/3000], Loss: 0.3076\nEpoch [640/3000], Loss: 0.3076\nEpoch [650/3000], Loss: 0.3073\nEpoch [660/3000], Loss: 0.3077\nEpoch [670/3000], Loss: 0.3077\nEpoch [680/3000], Loss: 0.3073\nEpoch [690/3000], Loss: 0.3073\nEpoch [700/3000], Loss: 0.3074\nEpoch [710/3000], Loss: 0.3074\nEpoch [720/3000], Loss: 0.3070\nEpoch [730/3000], Loss: 0.3073\nEpoch [740/3000], Loss: 0.3071\nEpoch [750/3000], Loss: 0.3071\nEpoch [760/3000], Loss: 0.3069\nEpoch [770/3000], Loss: 0.3067\nEpoch [780/3000], Loss: 0.3069\nEpoch [790/3000], Loss: 0.3071\nEpoch [800/3000], Loss: 0.3070\nEpoch [810/3000], Loss: 0.3071\nEpoch [820/3000], Loss: 0.3067\nEpoch [830/3000], Loss: 0.3068\nEpoch [840/3000], Loss: 0.3071\nEpoch [850/3000], Loss: 0.3071\nEpoch [860/3000], Loss: 0.3066\nEpoch [870/3000], Loss: 0.3066\nEpoch [880/3000], Loss: 0.3068\nEpoch [890/3000], Loss: 0.3066\nEpoch [900/3000], Loss: 0.3071\nEpoch [910/3000], Loss: 0.3066\nEpoch [920/3000], Loss: 0.3066\nEpoch [930/3000], Loss: 0.3063\nEpoch [940/3000], Loss: 0.3067\nEpoch [950/3000], Loss: 0.3063\nEpoch [960/3000], Loss: 0.3062\nEpoch [970/3000], Loss: 0.3069\nEpoch [980/3000], Loss: 0.3064\nEpoch [990/3000], Loss: 0.3064\nEpoch [1000/3000], Loss: 0.3068\nEpoch [1010/3000], Loss: 0.3064\nEpoch [1020/3000], Loss: 0.3065\nEpoch [1030/3000], Loss: 0.3060\nEpoch [1040/3000], Loss: 0.3059\nEpoch [1050/3000], Loss: 0.3060\nEpoch [1060/3000], Loss: 0.3064\nEpoch [1070/3000], Loss: 0.3064\nEpoch [1080/3000], Loss: 0.3062\nEpoch [1090/3000], Loss: 0.3062\nEpoch [1100/3000], Loss: 0.3059\nEpoch [1110/3000], Loss: 0.3060\nEpoch [1120/3000], Loss: 0.3062\nEpoch [1130/3000], Loss: 0.3055\nEpoch [1140/3000], Loss: 0.3060\nEpoch [1150/3000], Loss: 0.3063\nEpoch [1160/3000], Loss: 0.3061\nEpoch [1170/3000], Loss: 0.3058\nEpoch [1180/3000], Loss: 0.3062\nEpoch [1190/3000], Loss: 0.3055\nEpoch [1200/3000], Loss: 0.3060\nEpoch [1210/3000], Loss: 0.3051\nEpoch [1220/3000], Loss: 0.3060\nEpoch [1230/3000], Loss: 0.3059\nEpoch [1240/3000], Loss: 0.3057\nEpoch [1250/3000], Loss: 0.3057\nEpoch [1260/3000], Loss: 0.3059\nEpoch [1270/3000], Loss: 0.3057\nEpoch [1280/3000], Loss: 0.3060\nEpoch [1290/3000], Loss: 0.3057\nEpoch [1300/3000], Loss: 0.3057\nEpoch [1310/3000], Loss: 0.3056\nEpoch [1320/3000], Loss: 0.3058\nEpoch [1330/3000], Loss: 0.3058\nEpoch [1340/3000], Loss: 0.3057\nEpoch [1350/3000], Loss: 0.3054\nEpoch [1360/3000], Loss: 0.3055\nEpoch [1370/3000], Loss: 0.3054\nEpoch [1380/3000], Loss: 0.3060\nEpoch [1390/3000], Loss: 0.3057\nEpoch [1400/3000], Loss: 0.3059\nEpoch [1410/3000], Loss: 0.3054\nEpoch [1420/3000], Loss: 0.3058\nEpoch [1430/3000], Loss: 0.3055\nEpoch [1440/3000], Loss: 0.3057\nEpoch [1450/3000], Loss: 0.3054\nEpoch [1460/3000], Loss: 0.3058\nEpoch [1470/3000], Loss: 0.3054\nEpoch [1480/3000], Loss: 0.3052\nEpoch [1490/3000], Loss: 0.3057\nEpoch [1500/3000], Loss: 0.3051\nEpoch [1510/3000], Loss: 0.3051\nEpoch [1520/3000], Loss: 0.3051\nEpoch [1530/3000], Loss: 0.3050\nEpoch [1540/3000], Loss: 0.3061\nEpoch [1550/3000], Loss: 0.3061\nEpoch [1560/3000], Loss: 0.3054\nEpoch [1570/3000], Loss: 0.3055\nEpoch [1580/3000], Loss: 0.3056\nEpoch [1590/3000], Loss: 0.3054\nEpoch [1600/3000], Loss: 0.3050\nEpoch [1610/3000], Loss: 0.3048\nEpoch [1620/3000], Loss: 0.3056\nEpoch [1630/3000], Loss: 0.3050\nEpoch [1640/3000], Loss: 0.3051\nEpoch [1650/3000], Loss: 0.3050\nEpoch [1660/3000], Loss: 0.3054\nEpoch [1670/3000], Loss: 0.3048\nEpoch [1680/3000], Loss: 0.3051\nEpoch [1690/3000], Loss: 0.3050\nEpoch [1700/3000], Loss: 0.3057\nEpoch [1710/3000], Loss: 0.3055\nEpoch [1720/3000], Loss: 0.3049\nEpoch [1730/3000], Loss: 0.3051\nEpoch [1740/3000], Loss: 0.3050\nEpoch [1750/3000], Loss: 0.3054\nEpoch [1760/3000], Loss: 0.3051\nEpoch [1770/3000], Loss: 0.3048\nEpoch [1780/3000], Loss: 0.3045\nEpoch [1790/3000], Loss: 0.3052\nEpoch [1800/3000], Loss: 0.3052\nEpoch [1810/3000], Loss: 0.3053\nEpoch [1820/3000], Loss: 0.3052\nEpoch [1830/3000], Loss: 0.3052\nEpoch [1840/3000], Loss: 0.3049\nEpoch [1850/3000], Loss: 0.3053\nEpoch [1860/3000], Loss: 0.3050\nEpoch [1870/3000], Loss: 0.3056\nEpoch [1880/3000], Loss: 0.3049\nEpoch [1890/3000], Loss: 0.3047\nEpoch [1900/3000], Loss: 0.3052\nEpoch [1910/3000], Loss: 0.3049\nEpoch [1920/3000], Loss: 0.3050\nEpoch [1930/3000], Loss: 0.3050\nEpoch [1940/3000], Loss: 0.3049\nEpoch [1950/3000], Loss: 0.3047\nEpoch [1960/3000], Loss: 0.3051\nEpoch [1970/3000], Loss: 0.3052\nEpoch [1980/3000], Loss: 0.3054\nEpoch [1990/3000], Loss: 0.3049\nEpoch [2000/3000], Loss: 0.3056\nEpoch [2010/3000], Loss: 0.3047\nEpoch [2020/3000], Loss: 0.3052\nEpoch [2030/3000], Loss: 0.3049\nEpoch [2040/3000], Loss: 0.3048\nEpoch [2050/3000], Loss: 0.3046\nEpoch [2060/3000], Loss: 0.3051\nEpoch [2070/3000], Loss: 0.3049\nEpoch [2080/3000], Loss: 0.3049\nEpoch [2090/3000], Loss: 0.3053\nEpoch [2100/3000], Loss: 0.3048\nEpoch [2110/3000], Loss: 0.3051\nEpoch [2120/3000], Loss: 0.3045\nEpoch [2130/3000], Loss: 0.3053\nEpoch [2140/3000], Loss: 0.3043\nEpoch [2150/3000], Loss: 0.3051\nEpoch [2160/3000], Loss: 0.3048\nEpoch [2170/3000], Loss: 0.3054\nEpoch [2180/3000], Loss: 0.3046\nEpoch [2190/3000], Loss: 0.3052\nEpoch [2200/3000], Loss: 0.3051\nEpoch [2210/3000], Loss: 0.3051\nEpoch [2220/3000], Loss: 0.3043\nEpoch [2230/3000], Loss: 0.3052\nEpoch [2240/3000], Loss: 0.3051\nEpoch [2250/3000], Loss: 0.3044\nEpoch [2260/3000], Loss: 0.3045\nEpoch [2270/3000], Loss: 0.3042\nEpoch [2280/3000], Loss: 0.3040\nEpoch [2290/3000], Loss: 0.3054\nEpoch [2300/3000], Loss: 0.3048\nEpoch [2310/3000], Loss: 0.3044\nEpoch [2320/3000], Loss: 0.3045\nEpoch [2330/3000], Loss: 0.3048\nEpoch [2340/3000], Loss: 0.3050\nEpoch [2350/3000], Loss: 0.3046\nEpoch [2360/3000], Loss: 0.3044\nEpoch [2370/3000], Loss: 0.3048\nEpoch [2380/3000], Loss: 0.3048\nEpoch [2390/3000], Loss: 0.3043\nEpoch [2400/3000], Loss: 0.3043\nEpoch [2410/3000], Loss: 0.3047\nEpoch [2420/3000], Loss: 0.3049\nEpoch [2430/3000], Loss: 0.3044\nEpoch [2440/3000], Loss: 0.3045\nEpoch [2450/3000], Loss: 0.3044\nEpoch [2460/3000], Loss: 0.3049\nEpoch [2470/3000], Loss: 0.3041\nEpoch [2480/3000], Loss: 0.3045\nEpoch [2490/3000], Loss: 0.3048\nEpoch [2500/3000], Loss: 0.3042\nEpoch [2510/3000], Loss: 0.3044\nEpoch [2520/3000], Loss: 0.3045\nEpoch [2530/3000], Loss: 0.3047\nEpoch [2540/3000], Loss: 0.3048\nEpoch [2550/3000], Loss: 0.3046\nEpoch [2560/3000], Loss: 0.3047\nEpoch [2570/3000], Loss: 0.3044\nEpoch [2580/3000], Loss: 0.3046\nEpoch [2590/3000], Loss: 0.3045\nEpoch [2600/3000], Loss: 0.3044\nEpoch [2610/3000], Loss: 0.3043\nEpoch [2620/3000], Loss: 0.3039\nEpoch [2630/3000], Loss: 0.3046\nEpoch [2640/3000], Loss: 0.3045\nEpoch [2650/3000], Loss: 0.3040\nEpoch [2660/3000], Loss: 0.3037\nEpoch [2670/3000], Loss: 0.3048\nEpoch [2680/3000], Loss: 0.3043\nEpoch [2690/3000], Loss: 0.3044\nEpoch [2700/3000], Loss: 0.3050\nEpoch [2710/3000], Loss: 0.3044\nEpoch [2720/3000], Loss: 0.3049\nEpoch [2730/3000], Loss: 0.3041\nEpoch [2740/3000], Loss: 0.3040\nEpoch [2750/3000], Loss: 0.3043\nEpoch [2760/3000], Loss: 0.3052\nEpoch [2770/3000], Loss: 0.3042\nEpoch [2780/3000], Loss: 0.3044\nEpoch [2790/3000], Loss: 0.3046\nEpoch [2800/3000], Loss: 0.3046\nEpoch [2810/3000], Loss: 0.3045\nEpoch [2820/3000], Loss: 0.3037\nEpoch [2830/3000], Loss: 0.3047\nEpoch [2840/3000], Loss: 0.3042\nEpoch [2850/3000], Loss: 0.3045\nEpoch [2860/3000], Loss: 0.3044\nEpoch [2870/3000], Loss: 0.3048\nEpoch [2880/3000], Loss: 0.3042\nEpoch [2890/3000], Loss: 0.3048\nEpoch [2900/3000], Loss: 0.3041\nEpoch [2910/3000], Loss: 0.3043\nEpoch [2920/3000], Loss: 0.3043\nEpoch [2930/3000], Loss: 0.3040\nEpoch [2940/3000], Loss: 0.3046\nEpoch [2950/3000], Loss: 0.3039\nEpoch [2960/3000], Loss: 0.3043\nEpoch [2970/3000], Loss: 0.3044\nEpoch [2980/3000], Loss: 0.3043\nEpoch [2990/3000], Loss: 0.3044\nEpoch [3000/3000], Loss: 0.3046\n","output_type":"stream"}],"execution_count":90},{"cell_type":"code","source":"# Set the model to evaluation mode (no gradient updates)\nmodel2.eval()\n\nwith torch.no_grad():  # No gradient calculation during inference\n    # Forward pass through the model\n    outputs = model2(X_test_tensor)\n\n    # Convert probabilities to binary labels (0 or 1)\n    predicted = (outputs >= 0.5).float()\n\n# Calculate accuracy\n# Move tensors to CPU for use with scikit-learn\ny_pred = predicted.cpu().numpy()  # Convert to NumPy array\ny_test_cpu = Y_test_tensor.cpu().numpy()  # Convert to NumPy array\n\n# Calculate accuracy using scikit-learn's accuracy_score\naccuracy = accuracy_score(y_pred, y_test_cpu)\nprint(f'Accuracy: {accuracy * 100:.2f}%')\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FouVhTMY0pHp","outputId":"a77b4f83-9f79-43f8-e4bc-126ce0017957","trusted":true,"execution":{"iopub.status.busy":"2024-12-11T06:23:34.108632Z","iopub.execute_input":"2024-12-11T06:23:34.108929Z","iopub.status.idle":"2024-12-11T06:23:34.120618Z","shell.execute_reply.started":"2024-12-11T06:23:34.108873Z","shell.execute_reply":"2024-12-11T06:23:34.119682Z"}},"outputs":[{"name":"stdout","text":"Accuracy: 88.54%\n","output_type":"stream"}],"execution_count":91},{"cell_type":"code","source":"# Set the model to evaluation mode (no gradient updates)\nmodel2.eval()\n\nwith torch.no_grad():  # No gradient calculation during inference\n    # Forward pass through the model\n    outputs = model2(new_x_test_tensor)\n\n    # Convert probabilities to binary labels (0 or 1)\n    predicted = (outputs >= 0.5).float()\n\n# Calculate accuracy\n# Move tensors to CPU for use with scikit-learn\ny_pred = predicted.cpu().numpy()  # Convert to NumPy array","metadata":{"id":"SS19AIhW0_pU","trusted":true,"execution":{"iopub.status.busy":"2024-12-11T06:23:34.121742Z","iopub.execute_input":"2024-12-11T06:23:34.122303Z","iopub.status.idle":"2024-12-11T06:23:34.131283Z","shell.execute_reply.started":"2024-12-11T06:23:34.122261Z","shell.execute_reply":"2024-12-11T06:23:34.130497Z"}},"outputs":[],"execution_count":92},{"cell_type":"code","source":"y_pred=pd.DataFrame(y_pred)\nprint(y_pred.value_counts())","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tQtP8fub2UUB","outputId":"1a6c7bb8-a4cd-4ecf-a2eb-2bcb0b16dfc4","trusted":true,"execution":{"iopub.status.busy":"2024-12-11T06:23:34.132590Z","iopub.execute_input":"2024-12-11T06:23:34.132824Z","iopub.status.idle":"2024-12-11T06:23:34.142681Z","shell.execute_reply.started":"2024-12-11T06:23:34.132800Z","shell.execute_reply":"2024-12-11T06:23:34.141904Z"}},"outputs":[{"name":"stdout","text":"0  \n0.0    50589\n1.0      481\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":93},{"cell_type":"code","source":"# Combine them into a single DataFrame\nmerged_df = pd.concat([ids, y_pred], axis=1)\nmerged_df = merged_df.rename(columns={0: 'Default'})\nmerged_df['Default']=merged_df['Default'].astype(int)\nmerged_df.to_csv('predictions1.csv',index=False)","metadata":{"id":"MSFE4fIq2Ynb","trusted":true,"execution":{"iopub.status.busy":"2024-12-11T06:23:42.466317Z","iopub.execute_input":"2024-12-11T06:23:42.466647Z","iopub.status.idle":"2024-12-11T06:23:42.509406Z","shell.execute_reply.started":"2024-12-11T06:23:42.466619Z","shell.execute_reply":"2024-12-11T06:23:42.508564Z"}},"outputs":[],"execution_count":94},{"cell_type":"code","source":"heyo=pd.read_csv('predictions1.csv')\nheyo['Default'].value_counts()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":178},"id":"31RbmaXD2-sH","outputId":"97a72d08-86dd-4007-9b29-8b9e703bc558","trusted":true,"execution":{"iopub.status.busy":"2024-12-11T06:23:43.963954Z","iopub.execute_input":"2024-12-11T06:23:43.964549Z","iopub.status.idle":"2024-12-11T06:23:43.996274Z","shell.execute_reply.started":"2024-12-11T06:23:43.964488Z","shell.execute_reply":"2024-12-11T06:23:43.995417Z"}},"outputs":[{"execution_count":95,"output_type":"execute_result","data":{"text/plain":"Default\n0    50589\n1      481\nName: count, dtype: int64"},"metadata":{}}],"execution_count":95},{"cell_type":"code","source":"# this gave an accuracy of 88.746 % on kaggle submission which is a significant improvement","metadata":{"id":"XmW798QT4inq","trusted":true,"execution":{"iopub.status.busy":"2024-12-11T06:24:50.864401Z","iopub.execute_input":"2024-12-11T06:24:50.864794Z","iopub.status.idle":"2024-12-11T06:24:50.868662Z","shell.execute_reply.started":"2024-12-11T06:24:50.864744Z","shell.execute_reply":"2024-12-11T06:24:50.867747Z"}},"outputs":[],"execution_count":96},{"cell_type":"markdown","source":"## Logistic Regression","metadata":{}},{"cell_type":"code","source":"# Logistic regression is essentially a single layer single neuron neural network that makes use of a sigmoid activation function\nlogistic_regression=nn.Sequential(\n    nn.Linear(16,1), # 16 input features -> 1 output neuron\n    nn.Sigmoid() # sigmoid activation function\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T06:36:25.137466Z","iopub.execute_input":"2024-12-11T06:36:25.137810Z","iopub.status.idle":"2024-12-11T06:36:25.143625Z","shell.execute_reply.started":"2024-12-11T06:36:25.137780Z","shell.execute_reply":"2024-12-11T06:36:25.142418Z"}},"outputs":[],"execution_count":97},{"cell_type":"code","source":"logistic_regression=logistic_regression.to(device)\noptimizer=optim.Adam(logistic_regression.parameters(),lr=0.01)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T06:36:53.950120Z","iopub.execute_input":"2024-12-11T06:36:53.950429Z","iopub.status.idle":"2024-12-11T06:36:53.955507Z","shell.execute_reply.started":"2024-12-11T06:36:53.950403Z","shell.execute_reply":"2024-12-11T06:36:53.954742Z"}},"outputs":[],"execution_count":98},{"cell_type":"code","source":"# train the new model\nnum_epochs = 1000  # Number of epochs for training\n\nfor epoch in range(num_epochs):\n    logistic_regression.train()  # Set model to training mode\n\n    # Forward pass\n    outputs = logistic_regression(X_train_tensor)  # Get predictions from model\n    loss = loss_function(outputs, Y_train_tensor)  # Calculate loss\n\n    # Backward pass\n    optimizer.zero_grad()  # Clear previous gradients\n    loss.backward()  # Backpropagate gradients\n    optimizer.step()  # Update weights\n\n    # Print loss every 10 epochs\n    if (epoch + 1) % 10 == 0:\n        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T06:37:42.634118Z","iopub.execute_input":"2024-12-11T06:37:42.635248Z","iopub.status.idle":"2024-12-11T06:37:43.514492Z","shell.execute_reply.started":"2024-12-11T06:37:42.635199Z","shell.execute_reply":"2024-12-11T06:37:43.513623Z"}},"outputs":[{"name":"stdout","text":"Epoch [10/1000], Loss: 0.3163\nEpoch [20/1000], Loss: 0.3163\nEpoch [30/1000], Loss: 0.3163\nEpoch [40/1000], Loss: 0.3163\nEpoch [50/1000], Loss: 0.3163\nEpoch [60/1000], Loss: 0.3163\nEpoch [70/1000], Loss: 0.3163\nEpoch [80/1000], Loss: 0.3163\nEpoch [90/1000], Loss: 0.3163\nEpoch [100/1000], Loss: 0.3163\nEpoch [110/1000], Loss: 0.3163\nEpoch [120/1000], Loss: 0.3163\nEpoch [130/1000], Loss: 0.3163\nEpoch [140/1000], Loss: 0.3163\nEpoch [150/1000], Loss: 0.3163\nEpoch [160/1000], Loss: 0.3163\nEpoch [170/1000], Loss: 0.3163\nEpoch [180/1000], Loss: 0.3163\nEpoch [190/1000], Loss: 0.3163\nEpoch [200/1000], Loss: 0.3163\nEpoch [210/1000], Loss: 0.3163\nEpoch [220/1000], Loss: 0.3163\nEpoch [230/1000], Loss: 0.3163\nEpoch [240/1000], Loss: 0.3163\nEpoch [250/1000], Loss: 0.3163\nEpoch [260/1000], Loss: 0.3163\nEpoch [270/1000], Loss: 0.3163\nEpoch [280/1000], Loss: 0.3163\nEpoch [290/1000], Loss: 0.3163\nEpoch [300/1000], Loss: 0.3163\nEpoch [310/1000], Loss: 0.3163\nEpoch [320/1000], Loss: 0.3163\nEpoch [330/1000], Loss: 0.3163\nEpoch [340/1000], Loss: 0.3163\nEpoch [350/1000], Loss: 0.3163\nEpoch [360/1000], Loss: 0.3163\nEpoch [370/1000], Loss: 0.3163\nEpoch [380/1000], Loss: 0.3163\nEpoch [390/1000], Loss: 0.3163\nEpoch [400/1000], Loss: 0.3163\nEpoch [410/1000], Loss: 0.3163\nEpoch [420/1000], Loss: 0.3163\nEpoch [430/1000], Loss: 0.3163\nEpoch [440/1000], Loss: 0.3163\nEpoch [450/1000], Loss: 0.3163\nEpoch [460/1000], Loss: 0.3163\nEpoch [470/1000], Loss: 0.3163\nEpoch [480/1000], Loss: 0.3163\nEpoch [490/1000], Loss: 0.3163\nEpoch [500/1000], Loss: 0.3163\nEpoch [510/1000], Loss: 0.3163\nEpoch [520/1000], Loss: 0.3163\nEpoch [530/1000], Loss: 0.3163\nEpoch [540/1000], Loss: 0.3163\nEpoch [550/1000], Loss: 0.3163\nEpoch [560/1000], Loss: 0.3163\nEpoch [570/1000], Loss: 0.3163\nEpoch [580/1000], Loss: 0.3163\nEpoch [590/1000], Loss: 0.3163\nEpoch [600/1000], Loss: 0.3163\nEpoch [610/1000], Loss: 0.3163\nEpoch [620/1000], Loss: 0.3163\nEpoch [630/1000], Loss: 0.3163\nEpoch [640/1000], Loss: 0.3163\nEpoch [650/1000], Loss: 0.3163\nEpoch [660/1000], Loss: 0.3163\nEpoch [670/1000], Loss: 0.3163\nEpoch [680/1000], Loss: 0.3163\nEpoch [690/1000], Loss: 0.3163\nEpoch [700/1000], Loss: 0.3163\nEpoch [710/1000], Loss: 0.3163\nEpoch [720/1000], Loss: 0.3163\nEpoch [730/1000], Loss: 0.3163\nEpoch [740/1000], Loss: 0.3163\nEpoch [750/1000], Loss: 0.3163\nEpoch [760/1000], Loss: 0.3163\nEpoch [770/1000], Loss: 0.3163\nEpoch [780/1000], Loss: 0.3163\nEpoch [790/1000], Loss: 0.3163\nEpoch [800/1000], Loss: 0.3163\nEpoch [810/1000], Loss: 0.3163\nEpoch [820/1000], Loss: 0.3163\nEpoch [830/1000], Loss: 0.3163\nEpoch [840/1000], Loss: 0.3163\nEpoch [850/1000], Loss: 0.3163\nEpoch [860/1000], Loss: 0.3163\nEpoch [870/1000], Loss: 0.3163\nEpoch [880/1000], Loss: 0.3163\nEpoch [890/1000], Loss: 0.3163\nEpoch [900/1000], Loss: 0.3163\nEpoch [910/1000], Loss: 0.3163\nEpoch [920/1000], Loss: 0.3163\nEpoch [930/1000], Loss: 0.3163\nEpoch [940/1000], Loss: 0.3163\nEpoch [950/1000], Loss: 0.3163\nEpoch [960/1000], Loss: 0.3163\nEpoch [970/1000], Loss: 0.3163\nEpoch [980/1000], Loss: 0.3163\nEpoch [990/1000], Loss: 0.3163\nEpoch [1000/1000], Loss: 0.3163\n","output_type":"stream"}],"execution_count":100},{"cell_type":"code","source":"# Set the model to evaluation mode (no gradient updates)\nlogistic_regression.eval()\n\nwith torch.no_grad():  # No gradient calculation during inference\n    # Forward pass through the model\n    outputs = logistic_regression(X_test_tensor)\n\n    # Convert probabilities to binary labels (0 or 1)\n    predicted = (outputs >= 0.5).float()\n\n# Calculate accuracy\n# Move tensors to CPU for use with scikit-learn\ny_pred = predicted.cpu().numpy()  # Convert to NumPy array\ny_test_cpu = Y_test_tensor.cpu().numpy()  # Convert to NumPy array\n\n# Calculate accuracy using scikit-learn's accuracy_score\naccuracy = accuracy_score(y_pred, y_test_cpu)\nprint(f'Accuracy: {accuracy * 100:.2f}%')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T06:38:13.356702Z","iopub.execute_input":"2024-12-11T06:38:13.357054Z","iopub.status.idle":"2024-12-11T06:38:13.368706Z","shell.execute_reply.started":"2024-12-11T06:38:13.357021Z","shell.execute_reply":"2024-12-11T06:38:13.367863Z"}},"outputs":[{"name":"stdout","text":"Accuracy: 88.43%\n","output_type":"stream"}],"execution_count":101},{"cell_type":"code","source":"# Set the model to evaluation mode (no gradient updates)\nlogistic_regression.eval()\n\nwith torch.no_grad():  # No gradient calculation during inference\n    # Forward pass through the model\n    outputs = logistic_regression(new_x_test_tensor)\n\n    # Convert probabilities to binary labels (0 or 1)\n    predicted = (outputs >= 0.5).float()\n\n# Calculate accuracy\n# Move tensors to CPU for use with scikit-learn\ny_pred = predicted.cpu().numpy()  # Convert to NumPy array","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T06:38:31.346703Z","iopub.execute_input":"2024-12-11T06:38:31.347059Z","iopub.status.idle":"2024-12-11T06:38:31.352515Z","shell.execute_reply.started":"2024-12-11T06:38:31.347026Z","shell.execute_reply":"2024-12-11T06:38:31.351751Z"}},"outputs":[],"execution_count":102},{"cell_type":"code","source":"y_pred=pd.DataFrame(y_pred)\nprint(y_pred.value_counts())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T06:38:39.511350Z","iopub.execute_input":"2024-12-11T06:38:39.511684Z","iopub.status.idle":"2024-12-11T06:38:39.519840Z","shell.execute_reply.started":"2024-12-11T06:38:39.511654Z","shell.execute_reply":"2024-12-11T06:38:39.519045Z"}},"outputs":[{"name":"stdout","text":"0  \n0.0    50749\n1.0      321\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":103},{"cell_type":"code","source":"# Combine them into a single DataFrame\nmerged_df = pd.concat([ids, y_pred], axis=1)\nmerged_df = merged_df.rename(columns={0: 'Default'})\nmerged_df['Default']=merged_df['Default'].astype(int)\nmerged_df.to_csv('predictions_lr.csv',index=False)\nheyo=pd.read_csv('predictions_lr.csv')\nheyo['Default'].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T06:39:11.191312Z","iopub.execute_input":"2024-12-11T06:39:11.191610Z","iopub.status.idle":"2024-12-11T06:39:11.258243Z","shell.execute_reply.started":"2024-12-11T06:39:11.191584Z","shell.execute_reply":"2024-12-11T06:39:11.257545Z"}},"outputs":[{"execution_count":105,"output_type":"execute_result","data":{"text/plain":"Default\n0    50749\n1      321\nName: count, dtype: int64"},"metadata":{}}],"execution_count":105},{"cell_type":"code","source":"# gave 88.586% accuracy on kaggle submission .","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Scikit Learn's Logistic Regression","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T07:42:22.543966Z","iopub.execute_input":"2024-12-11T07:42:22.545108Z","iopub.status.idle":"2024-12-11T07:42:22.621456Z","shell.execute_reply.started":"2024-12-11T07:42:22.545069Z","shell.execute_reply":"2024-12-11T07:42:22.620724Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"lr=LogisticRegression()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T07:42:24.096640Z","iopub.execute_input":"2024-12-11T07:42:24.097335Z","iopub.status.idle":"2024-12-11T07:42:24.101332Z","shell.execute_reply.started":"2024-12-11T07:42:24.097302Z","shell.execute_reply":"2024-12-11T07:42:24.100314Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"lr.fit(scaled_X_train,Y_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T07:43:05.599290Z","iopub.execute_input":"2024-12-11T07:43:05.599966Z","iopub.status.idle":"2024-12-11T07:43:05.774506Z","shell.execute_reply.started":"2024-12-11T07:43:05.599934Z","shell.execute_reply":"2024-12-11T07:43:05.772403Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"LogisticRegression()","text/html":"<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"y_pred=lr.predict(scaled_X_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T07:43:07.542425Z","iopub.execute_input":"2024-12-11T07:43:07.543260Z","iopub.status.idle":"2024-12-11T07:43:07.550297Z","shell.execute_reply.started":"2024-12-11T07:43:07.543226Z","shell.execute_reply":"2024-12-11T07:43:07.548232Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"print(accuracy_score(y_pred,Y_test))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T07:43:08.943676Z","iopub.execute_input":"2024-12-11T07:43:08.944339Z","iopub.status.idle":"2024-12-11T07:43:08.955123Z","shell.execute_reply.started":"2024-12-11T07:43:08.944307Z","shell.execute_reply":"2024-12-11T07:43:08.954259Z"}},"outputs":[{"name":"stdout","text":"0.8843254356765224\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"y_pred_sub=lr.predict(scaled_new_X_test)\ny_pred_sub=pd.DataFrame(y_pred_sub)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T07:43:10.223283Z","iopub.execute_input":"2024-12-11T07:43:10.223862Z","iopub.status.idle":"2024-12-11T07:43:10.231521Z","shell.execute_reply.started":"2024-12-11T07:43:10.223829Z","shell.execute_reply":"2024-12-11T07:43:10.229568Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# Combine them into a single DataFrame\nmerged_df = pd.concat([ids, y_pred_sub], axis=1)\nmerged_df = merged_df.rename(columns={0: 'Default'})\nmerged_df['Default']=merged_df['Default'].astype(int)\nmerged_df.to_csv('predictions_lr_sklearn.csv',index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T07:43:11.571821Z","iopub.execute_input":"2024-12-11T07:43:11.572170Z","iopub.status.idle":"2024-12-11T07:43:11.616312Z","shell.execute_reply.started":"2024-12-11T07:43:11.572140Z","shell.execute_reply":"2024-12-11T07:43:11.615653Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"merged_df['Default'].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T07:43:13.529808Z","iopub.execute_input":"2024-12-11T07:43:13.530457Z","iopub.status.idle":"2024-12-11T07:43:13.541486Z","shell.execute_reply.started":"2024-12-11T07:43:13.530423Z","shell.execute_reply":"2024-12-11T07:43:13.540531Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"Default\n0    50749\n1      321\nName: count, dtype: int64"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"# You can see that sklearn's logistic regression and our logistic regression implemented as a single neuron neural network worked exactly the same","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T07:43:40.526224Z","iopub.execute_input":"2024-12-11T07:43:40.526879Z","iopub.status.idle":"2024-12-11T07:43:40.530483Z","shell.execute_reply.started":"2024-12-11T07:43:40.526847Z","shell.execute_reply":"2024-12-11T07:43:40.529561Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"## Support Vector Machines","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVC","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T07:18:20.257596Z","iopub.execute_input":"2024-12-12T07:18:20.257921Z","iopub.status.idle":"2024-12-12T07:18:20.340054Z","shell.execute_reply.started":"2024-12-12T07:18:20.257890Z","shell.execute_reply":"2024-12-12T07:18:20.339184Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"classifier=SVC()\n# Sklearn's SVM was not able to run due to the large input size. We ha","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T07:18:22.183082Z","iopub.execute_input":"2024-12-12T07:18:22.183877Z","iopub.status.idle":"2024-12-12T07:18:22.187952Z","shell.execute_reply.started":"2024-12-12T07:18:22.183845Z","shell.execute_reply":"2024-12-12T07:18:22.186967Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"import cuml\nprint(\"cuML is available!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T07:18:25.241604Z","iopub.execute_input":"2024-12-12T07:18:25.241919Z","iopub.status.idle":"2024-12-12T07:18:30.515110Z","shell.execute_reply.started":"2024-12-12T07:18:25.241893Z","shell.execute_reply":"2024-12-12T07:18:30.514154Z"}},"outputs":[{"name":"stdout","text":"cuML is available!\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"#### RBF kernel","metadata":{}},{"cell_type":"code","source":"from cuml.svm import SVC\nimport cudf\n# Replace X_data and y_data with your dataset\nX_gpu = cudf.DataFrame(scaled_X_train)\ny_gpu = cudf.Series(Y_train)\n\n# Train and predict using cuML's SVM\nmodel = SVC(kernel='rbf', C=1.4, gamma='scale')\nmodel.fit(X_gpu, y_gpu)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T07:24:28.432784Z","iopub.execute_input":"2024-12-12T07:24:28.433385Z","iopub.status.idle":"2024-12-12T07:28:24.332153Z","shell.execute_reply.started":"2024-12-12T07:24:28.433338Z","shell.execute_reply":"2024-12-12T07:28:24.331354Z"}},"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"SVC()","text/html":"<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div>"},"metadata":{}}],"execution_count":35},{"cell_type":"code","source":"X_gpu_test=cudf.DataFrame(scaled_X_test)\npreds=model.predict(X_gpu_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T07:29:32.904508Z","iopub.execute_input":"2024-12-12T07:29:32.904958Z","iopub.status.idle":"2024-12-12T07:29:33.975600Z","shell.execute_reply.started":"2024-12-12T07:29:32.904916Z","shell.execute_reply":"2024-12-12T07:29:33.974626Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"preds=preds.to_numpy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T07:29:37.342952Z","iopub.execute_input":"2024-12-12T07:29:37.343305Z","iopub.status.idle":"2024-12-12T07:29:37.348419Z","shell.execute_reply.started":"2024-12-12T07:29:37.343274Z","shell.execute_reply":"2024-12-12T07:29:37.347557Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"print(accuracy_score(preds,Y_test))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T07:29:39.403115Z","iopub.execute_input":"2024-12-12T07:29:39.403923Z","iopub.status.idle":"2024-12-12T07:29:39.417397Z","shell.execute_reply.started":"2024-12-12T07:29:39.403870Z","shell.execute_reply":"2024-12-12T07:29:39.416272Z"}},"outputs":[{"name":"stdout","text":"0.8829792441746622\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"y_pred=model.predict(scaled_new_X_test)\ny_pred=pd.DataFrame(y_pred)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T07:29:41.347651Z","iopub.execute_input":"2024-12-12T07:29:41.347958Z","iopub.status.idle":"2024-12-12T07:29:42.741359Z","shell.execute_reply.started":"2024-12-12T07:29:41.347933Z","shell.execute_reply":"2024-12-12T07:29:42.740426Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"# Combine them into a single DataFrame\nmerged_df = pd.concat([ids, y_pred], axis=1)\nmerged_df = merged_df.rename(columns={0: 'Default'})\nmerged_df['Default']=merged_df['Default'].astype(int)\nmerged_df.to_csv('predictions_svm.csv',index=False)\nheyo=pd.read_csv('predictions_svm.csv')\nheyo['Default'].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T07:29:45.398066Z","iopub.execute_input":"2024-12-12T07:29:45.398425Z","iopub.status.idle":"2024-12-12T07:29:45.471581Z","shell.execute_reply.started":"2024-12-12T07:29:45.398393Z","shell.execute_reply":"2024-12-12T07:29:45.470742Z"}},"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"Default\n0    51067\n1        3\nName: count, dtype: int64"},"metadata":{}}],"execution_count":40},{"cell_type":"code","source":"# This model gave a relatively low accuaracy of 88.453% on submission","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T08:57:39.463349Z","iopub.execute_input":"2024-12-11T08:57:39.464467Z","iopub.status.idle":"2024-12-11T08:57:39.468712Z","shell.execute_reply.started":"2024-12-11T08:57:39.464381Z","shell.execute_reply":"2024-12-11T08:57:39.467568Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"from cuml.svm import SVC\nimport cudf\n# Replace X_data and y_data with your dataset\nX_gpu = cudf.DataFrame(scaled_X_train)\ny_gpu = cudf.Series(Y_train)\n\n# Train and predict using cuML's SVM\nmodel = SVC(kernel='rbf', C=0.2, gamma='scale')\nmodel.fit(X_gpu, y_gpu)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T08:55:44.315299Z","iopub.execute_input":"2024-12-11T08:55:44.316006Z","iopub.status.idle":"2024-12-11T08:57:18.933258Z","shell.execute_reply.started":"2024-12-11T08:55:44.315970Z","shell.execute_reply":"2024-12-11T08:57:18.932102Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"SVC()","text/html":"<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div>"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"preds=model.predict(X_gpu_test)\npreds=preds.to_numpy()\nprint(accuracy_score(preds,Y_test))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T08:58:09.510968Z","iopub.execute_input":"2024-12-11T08:58:09.511279Z","iopub.status.idle":"2024-12-11T08:58:10.575580Z","shell.execute_reply.started":"2024-12-11T08:58:09.511253Z","shell.execute_reply":"2024-12-11T08:58:10.574832Z"}},"outputs":[{"name":"stdout","text":"0.8829302917564128\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"y_pred=model.predict(scaled_new_X_test)\ny_pred=pd.DataFrame(y_pred)\n# Combine them into a single DataFrame\nmerged_df = pd.concat([ids, y_pred], axis=1)\nmerged_df = merged_df.rename(columns={0: 'Default'})\nmerged_df['Default']=merged_df['Default'].astype(int)\nmerged_df.to_csv('predictions_svm1.csv',index=False)\nheyo=pd.read_csv('predictions_svm1.csv')\nheyo['Default'].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T08:58:30.793046Z","iopub.execute_input":"2024-12-11T08:58:30.794121Z","iopub.status.idle":"2024-12-11T08:58:32.163911Z","shell.execute_reply.started":"2024-12-11T08:58:30.794082Z","shell.execute_reply":"2024-12-11T08:58:32.162995Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"Default\n0    51070\nName: count, dtype: int64"},"metadata":{}}],"execution_count":21},{"cell_type":"markdown","source":"#### Polynomial Kernel","metadata":{}},{"cell_type":"code","source":"# Train and predict using cuML's SVM\nmodel = SVC(kernel='poly', C=1.0, gamma='scale',coef0=1,degree=3)\nmodel.fit(X_gpu, y_gpu)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T07:34:31.361480Z","iopub.execute_input":"2024-12-12T07:34:31.361822Z","iopub.status.idle":"2024-12-12T07:35:07.809088Z","shell.execute_reply.started":"2024-12-12T07:34:31.361792Z","shell.execute_reply":"2024-12-12T07:35:07.808248Z"}},"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"SVC()","text/html":"<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div>"},"metadata":{}}],"execution_count":41},{"cell_type":"code","source":"preds=model.predict(X_gpu_test)\npreds=preds.to_numpy()\nprint(accuracy_score(preds,Y_test))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T08:01:27.346562Z","iopub.execute_input":"2024-12-12T08:01:27.347264Z","iopub.status.idle":"2024-12-12T08:01:29.648714Z","shell.execute_reply.started":"2024-12-12T08:01:27.347220Z","shell.execute_reply":"2024-12-12T08:01:29.647763Z"}},"outputs":[{"name":"stdout","text":"0.8829302917564128\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"y_pred=model.predict(scaled_new_X_test)\ny_pred=pd.DataFrame(y_pred)\n# Combine them into a single DataFrame\nmerged_df = pd.concat([ids, y_pred], axis=1)\nmerged_df = merged_df.rename(columns={0: 'Default'})\nmerged_df['Default']=merged_df['Default'].astype(int)\nmerged_df.to_csv('predictions_svm2.csv',index=False)\nheyo=pd.read_csv('predictions_svm2.csv')\nheyo['Default'].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T08:01:32.771796Z","iopub.execute_input":"2024-12-12T08:01:32.772629Z","iopub.status.idle":"2024-12-12T08:01:35.676743Z","shell.execute_reply.started":"2024-12-12T08:01:32.772594Z","shell.execute_reply":"2024-12-12T08:01:35.675846Z"}},"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"Default\n0    51070\nName: count, dtype: int64"},"metadata":{}}],"execution_count":43},{"cell_type":"code","source":"# Even polynomial Kernel didn't give the best results. Gave 88.447% accuracy on Kaggle submission","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T08:03:22.326193Z","iopub.execute_input":"2024-12-12T08:03:22.326904Z","iopub.status.idle":"2024-12-12T08:03:22.330535Z","shell.execute_reply.started":"2024-12-12T08:03:22.326873Z","shell.execute_reply":"2024-12-12T08:03:22.329597Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"# Train and predict using cuML's SVM\nmodel = SVC(kernel='poly', C=1.8, gamma='scale',coef0=1,degree=4)\nmodel.fit(X_gpu, y_gpu)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T09:11:10.390092Z","iopub.execute_input":"2024-12-11T09:11:10.391226Z","iopub.status.idle":"2024-12-11T09:26:46.869320Z","shell.execute_reply.started":"2024-12-11T09:11:10.391188Z","shell.execute_reply":"2024-12-11T09:26:46.868325Z"}},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"SVC()","text/html":"<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div>"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"preds=model.predict(X_gpu_test)\npreds=preds.to_numpy()\nprint(accuracy_score(preds,Y_test))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T09:56:27.701778Z","iopub.execute_input":"2024-12-11T09:56:27.702590Z","iopub.status.idle":"2024-12-11T09:56:31.050782Z","shell.execute_reply.started":"2024-12-11T09:56:27.702557Z","shell.execute_reply":"2024-12-11T09:56:31.049887Z"}},"outputs":[{"name":"stdout","text":"0.8829302917564128\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}